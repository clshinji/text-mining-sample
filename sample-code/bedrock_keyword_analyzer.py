#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Amazon Bedrock ã‚’ä½¿ç”¨ã—ãŸãƒ†ã‚­ã‚¹ãƒˆåˆ†æã‚µãƒ³ãƒ—ãƒ«
Claude 4 Sonnet ã§ç‰¹å¾´çš„ãªå˜èªã‚’æŠ½å‡ºã—ã€å‡ºç¾å›æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆã—ã¾ã™
"""

import boto3
import json
import re
from datetime import datetime
from wordcloud import WordCloud
import matplotlib.pyplot as plt

def extract_json_from_text(text):
    """
    ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰JSONã‚’æŠ½å‡ºã™ã‚‹é–¢æ•°
    LLMã®å‡ºåŠ›ã«JSONä»¥å¤–ã®ãƒ†ã‚­ã‚¹ãƒˆãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã«å¯¾å¿œ
    """
    # è¤‡æ•°ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã§JSONã‚’æ¢ã™
    patterns = [
        # ãƒ‘ã‚¿ãƒ¼ãƒ³1: æœ€åˆã®{ã‹ã‚‰æœ€å¾Œã®}ã¾ã§
        r'\{.*\}',
        # ãƒ‘ã‚¿ãƒ¼ãƒ³2: ```json ãƒ–ãƒ­ãƒƒã‚¯å†…
        r'```json\s*(\{.*?\})\s*```',
        # ãƒ‘ã‚¿ãƒ¼ãƒ³3: ``` ãƒ–ãƒ­ãƒƒã‚¯å†…
        r'```\s*(\{.*?\})\s*```'
    ]
    
    for pattern in patterns:
        matches = re.findall(pattern, text, re.DOTALL)
        for match in matches:
            try:
                # JSONã¨ã—ã¦è§£æã‚’è©¦è¡Œ
                parsed = json.loads(match)
                return parsed
            except json.JSONDecodeError:
                continue
    
    # è¡Œã”ã¨ã«åˆ†å‰²ã—ã¦JSONã‚‰ã—ã„è¡Œã‚’æ¢ã™
    lines = text.split('\n')
    json_lines = []
    in_json = False
    brace_count = 0
    
    for line in lines:
        stripped = line.strip()
        if stripped.startswith('{'):
            in_json = True
            json_lines = [line]
            brace_count = line.count('{') - line.count('}')
        elif in_json:
            json_lines.append(line)
            brace_count += line.count('{') - line.count('}')
            if brace_count <= 0:
                break
    
    if json_lines:
        try:
            json_text = '\n'.join(json_lines)
            return json.loads(json_text)
        except json.JSONDecodeError:
            pass
    
    return None

def create_wordcloud(word_frequency, top_n=10):
    """
    å˜èªã®å‡ºç¾å›æ•°ã‹ã‚‰ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã‚’ç”Ÿæˆã™ã‚‹é–¢æ•°
    """
    # ä¸Šä½Nä½ã¾ã§ã®å˜èªã‚’å–å¾—
    top_words = dict(list(word_frequency.items())[:top_n])
    
    if not top_words:
        print("âŒ ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ç”¨ã®ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
        return
    
    print(f"ğŸ¨ ä¸Šä½{top_n}ä½ã¾ã§ã®å˜èªã§ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã‚’ç”Ÿæˆä¸­...")
    
    try:
        # æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’æŒ‡å®š
        font_path = "./MEIRYO.TTC"
        
        # ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã‚’ç”Ÿæˆ
        wordcloud = WordCloud(
            width=800,
            height=400,
            background_color='white',
            max_words=top_n,
            relative_scaling=0.5,
            colormap='viridis',
            font_path=font_path  # MEIRYOãƒ•ã‚©ãƒ³ãƒˆã‚’æŒ‡å®š
        ).generate_from_frequencies(top_words)
        
        # ãƒ—ãƒ­ãƒƒãƒˆè¨­å®šï¼ˆæ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆå¯¾å¿œï¼‰
        plt.rcParams['font.family'] = 'DejaVu Sans'  # è‹±èªéƒ¨åˆ†ç”¨
        plt.figure(figsize=(10, 5))
        plt.imshow(wordcloud, interpolation='bilinear')
        plt.axis('off')
        plt.title(f'Word Frequency Ranking Top {top_n}', fontsize=16, pad=20)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        output_file = "wordcloud_result.png"
        plt.savefig(output_file, bbox_inches='tight', dpi=300)
        print(f"âœ… ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã‚’ {output_file} ã«ä¿å­˜ã—ã¾ã—ãŸ")
        
        # ç”»é¢ã«è¡¨ç¤º
        plt.show()
        
    except FileNotFoundError:
        print("âŒ MEIRYO.TTCãƒ•ã‚©ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        print("ğŸ’¡ ã‚«ãƒ¬ãƒ³ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«MEIRYO.TTCãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„")
    except Exception as e:
        print(f"âŒ ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ç”Ÿæˆã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

def analyze_text_with_bedrock():
    """
    ãƒ¡ã‚¤ãƒ³åˆ†æé–¢æ•°ï¼šãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€Bedrockã§åˆ†æã—ã€çµæœã‚’å‡ºåŠ›
    """
    
    # 1. è¨­å®š
    # Claude 4 Sonnetç”¨ã®æ¨è«–ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«IDã‚’ä½¿ç”¨
    MODEL_ID = "us.anthropic.claude-sonnet-4-20250514-v1:0"
    REGION = "us-east-1"
    
    print("ğŸš€ Amazon Bedrock ãƒ†ã‚­ã‚¹ãƒˆåˆ†æã‚’é–‹å§‹ã—ã¾ã™...")
    
    # 2. ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
    try:
        with open("../sample-text/sample.md", "r", encoding="utf-8") as file:
            sample_text = file.read()
        print("âœ… ã‚µãƒ³ãƒ—ãƒ«ãƒ†ã‚­ã‚¹ãƒˆã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
    except FileNotFoundError:
        print("âŒ sample.mdãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return
    
    # 3. ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’èª­ã¿è¾¼ã¿
    try:
        with open("../_work/llm.md", "r", encoding="utf-8") as file:
            prompt_template = file.read()
        print("âœ… ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
    except FileNotFoundError:
        print("âŒ llm.mdãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return
    
    # 4. Bedrockã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½œæˆ
    bedrock_runtime = boto3.client(
        service_name="bedrock-runtime",
        region_name=REGION
    )
    print("âœ… Bedrockã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’åˆæœŸåŒ–ã—ã¾ã—ãŸ")
    
    # 5. ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆï¼ˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã«ãƒ†ã‚­ã‚¹ãƒˆã‚’åŸ‹ã‚è¾¼ã¿ï¼‰
    prompt = prompt_template.replace("{{è§£æå¯¾è±¡ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’LLMã«æ¸¡ã™}}", sample_text)
    
    # 6. Bedrock converse_stream APIã‚’å‘¼ã³å‡ºã—
    try:
        print("ğŸ¤– Claude 4 Sonnet ã§åˆ†æä¸­...")
        
        response = bedrock_runtime.converse_stream(
            modelId=MODEL_ID,
            messages=[
                {
                    "role": "user",
                    "content": [
                        {
                            "text": prompt
                        }
                    ]
                }
            ],
            inferenceConfig={
                "maxTokens": 2000,
                "temperature": 0.1
            }
        )
        
        # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’åé›†
        llm_output = ""
        for event in response["stream"]:
            if "contentBlockDelta" in event:
                delta = event["contentBlockDelta"]["delta"]
                if "text" in delta:
                    llm_output += delta["text"]
        
        print("âœ… LLMã‹ã‚‰ã®å¿œç­”ã‚’å–å¾—ã—ã¾ã—ãŸ")
        print(f"ğŸ“„ LLMå‡ºåŠ›ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼: {llm_output[:200]}...")
        
    except Exception as e:
        print(f"âŒ Bedrock APIå‘¼ã³å‡ºã—ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return
    
    # 7. LLMã®å‡ºåŠ›ã‹ã‚‰JSONã‚’æŠ½å‡ºï¼ˆå …ç‰¢ãªãƒ‘ãƒ¼ã‚¹å‡¦ç†ï¼‰
    try:
        print("ğŸ” JSONå½¢å¼ã®ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºä¸­...")
        
        # å …ç‰¢ãªJSONæŠ½å‡ºã‚’å®Ÿè¡Œ
        keywords_data = extract_json_from_text(llm_output)
        
        if keywords_data is None:
            print("âŒ æœ‰åŠ¹ãªJSONãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            print(f"LLMå®Œå…¨å‡ºåŠ›:\n{llm_output}")
            return
        
        # resultsã‚­ãƒ¼ã‹ã‚‰å˜èªé…åˆ—ã‚’å–å¾—
        if "results" not in keywords_data:
            print("âŒ 'results'ã‚­ãƒ¼ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            print(f"å–å¾—ã—ãŸJSON: {keywords_data}")
            return
            
        keywords = keywords_data["results"]
        print(f"âœ… {len(keywords)}å€‹ã®ç‰¹å¾´çš„ãªå˜èªã‚’æŠ½å‡ºã—ã¾ã—ãŸ")
        print(f"ğŸ“ æŠ½å‡ºã•ã‚ŒãŸå˜èª: {keywords}")
        
    except Exception as e:
        print(f"âŒ JSONè§£æã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        print(f"LLMå®Œå…¨å‡ºåŠ›:\n{llm_output}")
        return
    
    # 8. å„å˜èªã®å‡ºç¾å›æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
    print("ğŸ” å˜èªã®å‡ºç¾å›æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆä¸­...")
    
    word_count = {}
    for keyword in keywords:
        # æ­£è¦è¡¨ç¾ã§å˜èªã®å‡ºç¾å›æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆï¼ˆå¤§æ–‡å­—å°æ–‡å­—ã‚’åŒºåˆ¥ã—ãªã„ï¼‰
        pattern = re.escape(keyword)
        matches = re.findall(pattern, sample_text, re.IGNORECASE)
        count = len(matches)
        word_count[keyword] = count
        print(f"  ğŸ“ '{keyword}': {count}å›")
    
    # 9. å‡ºç¾å›æ•°ã®å¤šã„é †ã«ä¸¦ã³æ›¿ãˆ
    print("ğŸ“Š å‡ºç¾å›æ•°ã®å¤šã„é †ã«ä¸¦ã³æ›¿ãˆä¸­...")
    sorted_word_count = dict(sorted(word_count.items(), key=lambda x: x[1], reverse=True))
    
    print("ğŸ† å‡ºç¾å›æ•°ãƒ©ãƒ³ã‚­ãƒ³ã‚°:")
    for i, (word, count) in enumerate(sorted_word_count.items(), 1):
        print(f"  {i}ä½: '{word}' - {count}å›")
    
    # 10. çµæœã‚’JSONå½¢å¼ã§å‡ºåŠ›
    result = {
        "analysis_timestamp": datetime.now().isoformat(),
        "source_file": "sample-text/sample.md",
        "model_used": MODEL_ID,
        "extracted_keywords": keywords,
        "word_frequency": sorted_word_count,  # ä¸¦ã³æ›¿ãˆæ¸ˆã¿ã®è¾æ›¸ã‚’ä½¿ç”¨
        "word_frequency_ranking": [
            {"rank": i, "word": word, "count": count} 
            for i, (word, count) in enumerate(sorted_word_count.items(), 1)
        ],
        "total_words_analyzed": len(keywords)
    }
    
    print("\n" + "="*50)
    print("ğŸ“Š åˆ†æçµæœï¼ˆJSONå½¢å¼ï¼‰")
    print("="*50)
    print(json.dumps(result, ensure_ascii=False, indent=2))
    
    # 11. çµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    output_file = "analysis_result.json"
    with open(output_file, "w", encoding="utf-8") as file:
        json.dump(result, file, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… çµæœã‚’ {output_file} ã«ä¿å­˜ã—ã¾ã—ãŸ")
    
    # 12. ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã‚’ç”Ÿæˆ
    create_wordcloud(sorted_word_count, top_n=10)
    
    print("ğŸ‰ åˆ†æå®Œäº†ï¼")

if __name__ == "__main__":
    analyze_text_with_bedrock()
