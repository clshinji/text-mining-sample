{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "732f59b9-b946-4c95-9c93-487e5828b01f",
   "metadata": {},
   "source": [
    "# ãƒ†ã‚­ã‚¹ãƒˆåˆ†æã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ¼ãƒ‰\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯ AWS ç’°å¢ƒã‚’åˆ©ç”¨ã—ã¦ãƒ†ã‚­ã‚¹ãƒˆåˆ†æã‚’ä½“é¨“ã™ã‚‹ãŸã‚ã®ã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ¼ãƒ‰é›†ã§ã™ã€‚\n",
    "Amazon Comprehendã€Amazon Bedrockã€MeCabã€Doc2Vecãªã©ã®æ§˜ã€…ãªæŠ€è¡“ã‚’ä½¿ç”¨ã—ã¦ã€æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆã®åˆ†æã‚’è¡Œã†ã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ¼ãƒ‰ã‚’æ²è¼‰ã—ã¦ã„ã¾ã™ã€‚\n",
    "\n",
    "## æ¦‚è¦\n",
    "\n",
    "æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆã®è‡ªç„¶è¨€èªå‡¦ç†ã«ãŠã„ã¦ã€ä»¥ä¸‹ã®åˆ†ææ‰‹æ³•ã‚’æ¯”è¼ƒãƒ»æ¤œè¨¼ã™ã‚‹ã“ã¨ã‚’ç›®çš„ã¨ã—ã¦ã„ã¾ã™ï¼š\n",
    "\n",
    "* **Amazon Comprehend**: AWSã®è‡ªç„¶è¨€èªå‡¦ç†ã‚µãƒ¼ãƒ“ã‚¹ã‚’ä½¿ç”¨ã—ãŸã‚­ãƒ¼ãƒ•ãƒ¬ãƒ¼ã‚ºæŠ½å‡ºã€ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£æ¤œå‡ºã€æ„Ÿæƒ…åˆ†æ\n",
    "* **MeCab**: æ—¥æœ¬èªå½¢æ…‹ç´ è§£æã«ã‚ˆã‚‹å˜èªé »åº¦åˆ†æ\n",
    "* **Doc2Vec**: æ–‡æ›¸ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã«ã‚ˆã‚‹é¡ä¼¼åº¦åˆ†æ\n",
    "*  **Amazon Bedrock**: Claude 4 Sonnetã‚’ä½¿ç”¨ã—ãŸç‰¹å¾´çš„ãªå˜èªæŠ½å‡º"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c6c3d1-888a-406f-b552-86b4781d1f04",
   "metadata": {},
   "source": [
    "å¿…è¦ãªãƒ•ã‚¡ã‚¤ãƒ«ã‚’ S3 ã‹ã‚‰ã‚³ãƒ”ãƒ¼ã™ã‚‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4e54c3-5887-446e-b904-c370fc0d3a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp s3://text-analysis-hanson-2025/sample-text sample-text  --recursive --include \"*.md\"\n",
    "!aws s3 cp s3://text-analysis-hanson-2025/templete templete  --recursive --include \"*.md\"\n",
    "!aws s3 cp s3://text-analysis-hanson-2025/requirements.txt requirements.txt\n",
    "!aws s3 cp s3://text-analysis-hanson-2025/MEIRYO.TTC MEIRYO.TTC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d6083f-12d8-404c-8d6e-c5127d87c18e",
   "metadata": {},
   "source": [
    "å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6603a425-53f9-4f9e-8565-e7bd9f878b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087fef27-fd1f-48ee-8e70-1709f3c18cb5",
   "metadata": {},
   "source": [
    "## Amazon Comprehend\n",
    "Python ã‚’ä½¿ã£ã¦ã€ Amazon Comprehend ã®ãƒ†ã‚­ã‚¹ãƒˆåˆ†æã‚’ä½“é¨“ã—ã¾ã™\n",
    "\n",
    "Amazon Comprehend ã¯ã€ä»¥ä¸‹ã®ç¨®é¡ã®ã‚¤ãƒ³ã‚µã‚¤ãƒˆã‚’åˆ†æã—ã¾ã™ã€‚\n",
    "\n",
    "* Entities â€” ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå†…ã§ã®äººç‰©ã€å ´æ‰€ã€ã‚¢ã‚¤ãƒ†ãƒ ã€å ´æ‰€ã¸ã®è¨€åŠ (åå‰)ã€‚\n",
    "* Key phrases â€” ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«ç¾ã‚Œã‚‹ãƒ•ãƒ¬ãƒ¼ã‚ºã€‚ä¾‹ãˆã°ã€ãƒã‚¹ã‚±ãƒƒãƒˆãƒœãƒ¼ãƒ«ã®è©¦åˆã«é–¢ã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã§ã¯ã€ãƒãƒ¼ãƒ ã®åå‰ã€ä¼šå ´ã®åå‰ã€æœ€çµ‚ã‚¹ã‚³ã‚¢ã‚’è¿”ã™ã“ã¨ãŒã§ãã¾ã™ã€‚\n",
    "* å€‹äººã‚’ç‰¹å®šã§ãã‚‹æƒ…å ± (PII) â€” ä½æ‰€ã‚„éŠ€è¡Œå£åº§ç•ªå·ã€é›»è©±ç•ªå·ãªã©ã€å€‹äººã‚’ç‰¹å®šã§ãã‚‹å€‹äººãƒ‡ãƒ¼ã‚¿ã€‚\n",
    "* Language â€” ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ä¸»è¦è¨€èªã€‚\n",
    "* Sentiment â€” ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ä¸»è¦ãªæ„Ÿæƒ…ã€‚è‚¯å®šçš„ã€ä¸­ç«‹ã€å¦å®šçš„ã€æ··åœ¨ã®ã„ãšã‚Œã‹ã‚‰ã«ã§ãã¾ã™ã€‚\n",
    "* Targeted sentiment â€” ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå†…ã®ç‰¹å®šã®ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã«é–¢é€£ã™ã‚‹æ„Ÿæƒ…ã€‚å‡ºç¾ã™ã‚‹å„ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã«å¯¾ã™ã‚‹æ„Ÿæƒ…ã¯ã€è‚¯å®šçš„ã€å¦å®šçš„ã€ä¸­ç«‹ã€æ··åœ¨ã®ã„ãšã‚Œã‹ã«ã§ãã¾ã™ã€‚\n",
    "* Syntax â€” ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå†…ã®å„å˜èªã®å“è©ã€‚\n",
    "\n",
    "ãªãŠã€æ—¥æœ¬èªã®å ´åˆã¯ `ã‚­ãƒ¼ãƒ•ãƒ¬ãƒ¼ã‚ºã®æ¤œå‡º` ãƒ» `ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã®æ¤œå‡º` ãƒ» `æ„Ÿæƒ…ã®åˆ†æ` ã®3ã¤ã«ã ã‘å¯¾å¿œã—ã¦ã„ã¾ã™ã€‚ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ Markdown å½¢å¼ï¼‰ã‚’èª­ã¿è¾¼ã‚“ã§ Amazon Comprehend ã«å…¥åŠ›ã—ã€3ãƒ‘ã‚¿ãƒ¼ãƒ³ã§ã®åˆ†æçµæœã‚’å‡ºåŠ›ã™ã‚‹ã‚ˆã†ãªã‚³ãƒ¼ãƒ‰ã«ãªã£ã¦ã„ã¾ã™ã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e534750-b0a7-404d-93f6-a4ac14d9c28a",
   "metadata": {},
   "source": [
    "Amazon Comprehend å‚è€ƒè³‡æ–™\n",
    "\n",
    "* ãƒ‡ãƒ™ãƒ­ãƒƒãƒ‘ãƒ¼ã‚¬ã‚¤ãƒ‰: [Amazon Comprehend ã¨ã¯](https://docs.aws.amazon.com/ja_jp/comprehend/latest/dg/what-is.html)\n",
    "* ãƒãƒ³ã‚ºã‚ªãƒ³: [AWS Hands-on for Beginners: AWS Managed AI/ML ã‚µãƒ¼ãƒ“ã‚¹ ã¯ã˜ã‚ã®ä¸€æ­©](https://pages.awscloud.com/JAPAN-event-OE-Hands-on-for-Beginners-AIML-2022-confirmation_003.html)\n",
    "* å‚è€ƒã«ãªã‚‹ãƒ–ãƒ­ã‚°: [ã€AWSã€‘ã€æ©Ÿæ¢°å­¦ç¿’ã€‘Amazon Comprehendã®Custom classification (ã‚«ã‚¹ã‚¿ãƒ åˆ†é¡) ã‚’ä½¿ã£ã¦æ—¥æœ¬èªã®ãƒ†ã‚­ã‚¹ãƒˆã‚’æ„Ÿæƒ…åˆ†æã—ã¦ã¿ãŸ](https://techblog.ap-com.co.jp/entry/2024/07/01/093000)\n",
    "* ã‚«ã‚¹ã‚¿ãƒ ãƒ¢ãƒ‡ãƒ«ã®ä¾‹: [Amazon Comprehend ã‚’ä½¿ç”¨ã—ã¦ã‚«ã‚¹ã‚¿ãƒ ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ãƒ¬ã‚³ã‚°ãƒŠã‚¤ã‚¶ãƒ¼ã‚’æ§‹ç¯‰ã™ã‚‹](https://aws.amazon.com/jp/blogs/news/build-a-custom-entity-recognizer-using-amazon-comprehend/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8962cc-90ea-4050-a0b7-56b1a8ded8a4",
   "metadata": {},
   "source": [
    "ã¾ãšã¯é–¢æ•°ã‚’å®šç¾©ã—ã¦ãŠãã¾ã™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4753d2fd-34fc-4610-8f0b-0bcbcae417bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "\n",
    "# --- è¨­å®š ---\n",
    "# SageMaker Notebookã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‹ã‚‰å®Ÿè¡Œã™ã‚‹å ´åˆã€ãƒ­ãƒ¼ãƒ«ã«Comprehendã¸ã®ã‚¢ã‚¯ã‚»ã‚¹æ¨©ãŒã‚ã‚Œã°\n",
    "# access_key, secret_key, region_name ã®è¨­å®šã¯ä¸è¦ã§ã™ã€‚\n",
    "# boto3ãŒè‡ªå‹•çš„ã«èªè¨¼æƒ…å ±ã‚’è§£æ±ºã—ã¾ã™ã€‚\n",
    "REGION_NAME = \"ap-northeast-1\" # ä¾‹: æ±äº¬ãƒªãƒ¼ã‚¸ãƒ§ãƒ³\n",
    "\n",
    "\n",
    "# --- é–¢æ•°å®šç¾© ---\n",
    "def analyze_text_with_comprehend(text, region_name):\n",
    "    \"\"\"\n",
    "    Amazon Comprehend ã‚’ä½¿ã£ã¦ãƒ†ã‚­ã‚¹ãƒˆåˆ†æã‚’å®Ÿè¡Œã™ã‚‹é–¢æ•°ã€‚\n",
    "    - ã‚­ãƒ¼ãƒ•ãƒ¬ãƒ¼ã‚ºã®æ¤œå‡º\n",
    "    - ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã®æ¤œå‡º\n",
    "    - æ„Ÿæƒ…ã®åˆ†æ\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Comprehend ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®ä½œæˆ\n",
    "        comprehend = boto3.client(\"comprehend\", region_name=region_name)\n",
    "        language_code = 'ja'\n",
    "\n",
    "        # --- 1. ã‚­ãƒ¼ãƒ•ãƒ¬ãƒ¼ã‚ºåˆ†æ ---\n",
    "        print(\"--- 1. Amazon Comprehendã«ã‚ˆã‚‹ã‚­ãƒ¼ãƒ•ãƒ¬ãƒ¼ã‚ºåˆ†æ ---\")\n",
    "        key_phrases_response = comprehend.detect_key_phrases(Text=text, LanguageCode=language_code)\n",
    "        print(\"æ¤œå‡ºã•ã‚ŒãŸã‚­ãƒ¼ãƒ•ãƒ¬ãƒ¼ã‚º:\")\n",
    "        if key_phrases_response['KeyPhrases']:\n",
    "            for phrase in key_phrases_response['KeyPhrases']:\n",
    "                print(f\"- {phrase['Text']} (ã‚¹ã‚³ã‚¢: {phrase['Score']:.4f})\")\n",
    "        else:\n",
    "            print(\"ã‚­ãƒ¼ãƒ•ãƒ¬ãƒ¼ã‚ºã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\")\n",
    "\n",
    "        print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "        # --- 2. ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£æ¤œå‡º ---\n",
    "        print(\"--- 2. Amazon Comprehendã«ã‚ˆã‚‹ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£æ¤œå‡º ---\")\n",
    "        entities_response = comprehend.detect_entities(Text=text, LanguageCode=language_code)\n",
    "        print(\"æ¤œå‡ºã•ã‚ŒãŸã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£:\")\n",
    "        if entities_response['Entities']:\n",
    "            for entity in entities_response['Entities']:\n",
    "                print(f\"- {entity['Text']} (ã‚¿ã‚¤ãƒ—: {entity['Type']}, ã‚¹ã‚³ã‚¢: {entity['Score']:.4f})\")\n",
    "        else:\n",
    "            print(\"ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\")\n",
    "\n",
    "        print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "        # --- 3. æ„Ÿæƒ…åˆ†æ ---\n",
    "        print(\"--- 3. Amazon Comprehendã«ã‚ˆã‚‹æ„Ÿæƒ…åˆ†æ ---\")\n",
    "        sentiment_response = comprehend.detect_sentiment(Text=text, LanguageCode=language_code)\n",
    "        print(f\"å…¨ä½“ã®æ„Ÿæƒ…: {sentiment_response['Sentiment']}\")\n",
    "        print(\"æ„Ÿæƒ…ã‚¹ã‚³ã‚¢:\")\n",
    "        for sentiment, score in sentiment_response['SentimentScore'].items():\n",
    "            print(f\"- {sentiment}: {score:.4f}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}\")\n",
    "\n",
    "\n",
    "print(\"æº–å‚™OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e3e531-17f9-4f5d-93f5-b9b0c82a355a",
   "metadata": {},
   "source": [
    "`æº–å‚™OK` ã¨å‡ºåŠ›ã•ã‚ŒãŸã‚‰ã€åˆ†æã§ãã‚‹çŠ¶æ…‹ã«ãªã£ã¦ã„ã¾ã™ã€‚ã‚‚ã—ã‚¨ãƒ©ãƒ¼ãŒè¡¨ç¤ºã•ã‚Œã‚‹å ´åˆã¯ã€å†…å®¹ã‚’ç¢ºèªã—ã¦ã‚³ãƒ¼ãƒ‰ã‚’ä¿®æ­£ã—ã¾ã—ã‚‡ã†ã€‚\n",
    "\n",
    "ç¶šã„ã¦ `Amazon Comprehend` ã‚’ä½¿ã£ãŸãƒ†ã‚­ã‚¹ãƒˆåˆ†æã‚’å®Ÿè¡Œã—ã¾ã™ã€‚æœ€åˆã¯ `sample-text/sample.md` ã‚’å¯¾è±¡ã«åˆ†æã™ã‚‹ã‚ˆã†ã«ãªã£ã¦ã„ã¾ã™ãŒã€æ…£ã‚Œã¦ããŸã‚‰ä»–ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚‚è©¦ã—ã¦ã¿ã¾ã—ã‚‡ã†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d9931d-06ab-4e63-96bf-68e5b0ed946d",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # ã‚µãƒ³ãƒ—ãƒ«ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€\n",
    "    # å¿…è¦ã«å¿œã˜ã¦ãƒ‘ã‚¹ã‚’èª¿æ•´ã—ã¦ãã ã•ã„ã€‚\n",
    "    file_path = \"sample-text/sample.md\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        sample_text = f.read()\n",
    "\n",
    "    # ãƒ†ã‚­ã‚¹ãƒˆãŒ5000ãƒã‚¤ãƒˆã‚’è¶…ãˆã¦ã„ã‚‹å ´åˆã¯åˆ†å‰²ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ãŒã€ä»Šå›ã¯ã‚µãƒ³ãƒ—ãƒ«ãªã®ã§å…¨ä½“ã‚’ä¸€åº¦ã«å‡¦ç†\n",
    "    if len(sample_text.encode('utf-8')) > 4800: # å¿µã®ãŸã‚å°‘ã—ä½™è£•ã‚’æŒãŸã›ã‚‹\n",
    "        print(\"è­¦å‘Š: ãƒ†ã‚­ã‚¹ãƒˆã‚µã‚¤ã‚ºãŒå¤§ãã„ãŸã‚ã€ä¸€éƒ¨ã‚’åˆ‡ã‚Šè©°ã‚ã¦åˆ†æã—ã¾ã™ã€‚\")\n",
    "        # UTF-8ã§ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã—ã¦ã‹ã‚‰ãƒã‚¤ãƒˆæ•°ã‚’åŸºæº–ã«åˆ‡ã‚Šè©°ã‚ã‚‹\n",
    "        encoded_text = sample_text.encode('utf-8')\n",
    "        truncated_encoded_text = encoded_text[:4800]\n",
    "        # ä¸å®Œå…¨ãªãƒãƒ«ãƒãƒã‚¤ãƒˆæ–‡å­—ã§çµ‚ã‚ã‚‰ãªã„ã‚ˆã†ã«ãƒ‡ã‚³ãƒ¼ãƒ‰ãƒ»å†ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰\n",
    "        sample_text = truncated_encoded_text.decode('utf-8', 'ignore')\n",
    "\n",
    "    # Comprehendã§åˆ†æã‚’å®Ÿè¡Œ\n",
    "    analyze_text_with_comprehend(sample_text, REGION_NAME)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ã‚¨ãƒ©ãƒ¼: ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ‘ã‚¹ã‚’ç¢ºèªã—ã¦ãã ã•ã„: {file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"ãƒ¡ã‚¤ãƒ³å‡¦ç†ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d035a371-43a7-4d14-a86f-a293c45fe76b",
   "metadata": {},
   "source": [
    "## MeCab\n",
    "\n",
    "æ¬¡ã¯ã€æ—¥æœ¬èªå½¢æ…‹ç´ è§£æã«ã‚ˆã‚‹å˜èªé »åº¦åˆ†æãŒã§ãã‚‹ `MeCab` ã‚’ä½¿ã£ãŸãƒ†ã‚­ã‚¹ãƒˆåˆ†æã‚’ä½“é¨“ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22773430-9fcf-4871-9462-efe86e8fb5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ã†ã¾ãå®Ÿè¡Œã§ããªã„å ´åˆã¯ã€ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã‚’å…ˆé ­ã® # ã‚’å‰Šé™¤ã—ã¦ã‹ã‚‰å®Ÿè¡Œã™ã‚‹\n",
    "# !python -m unidic download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be59186-19f6-4a4e-85dc-71969bbcbc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import MeCab\n",
    "import os\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# --- è¨­å®š ---\n",
    "# MeCabã®è¾æ›¸ãƒ‘ã‚¹ã€‚`!pip install ipadic` ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ãŸå ´åˆã€é€šå¸¸ã¯è‡ªå‹•ã§è§£æ±ºã•ã‚Œã¾ã™ãŒã€\n",
    "# ç’°å¢ƒã«ã‚ˆã£ã¦ã¯æ˜ç¤ºçš„ãªæŒ‡å®šãŒå¿…è¦ã§ã™ã€‚\n",
    "# ä¾‹: MECAB_ARGS = \"-d /usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd\"\n",
    "# ã“ã“ã§ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®è¾æ›¸(ipadic)ã‚’æƒ³å®šã—ã¾ã™ã€‚\n",
    "MECAB_ARGS = \"\"\n",
    "\n",
    "# åˆ†æå¯¾è±¡ã®ã‚µãƒ³ãƒ—ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«\n",
    "# ABSOLUTE_FILE_PATH = \"/workspaces/esio/amazon-comprehend/sample-text/sample.md\"\n",
    "\n",
    "# --- é–¢æ•°å®šç¾© ---\n",
    "\n",
    "def analyze_word_frequency_with_mecab(text):\n",
    "    \"\"\"\n",
    "    MeCabã‚’ä½¿ã£ã¦ãƒ†ã‚­ã‚¹ãƒˆã®å˜èªå‡ºç¾é »åº¦ã‚’åˆ†æã™ã‚‹é–¢æ•°ã€‚\n",
    "    å“è©ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã€åŸå‹åŒ–ã€ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰é™¤å»ãªã©ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ã‚’é©ç”¨ã—ã¾ã™ã€‚\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # MeCabã®åˆæœŸåŒ–\n",
    "        tagger = MeCab.Tagger(MECAB_ARGS)\n",
    "        tagger.parse('') # UnicodeDecodeErrorã‚’é¿ã‘ã‚‹ãŸã‚ã®ãŠã¾ã˜ãªã„\n",
    "\n",
    "        print(\"--- MeCabã«ã‚ˆã‚‹é »å‡ºå˜èªåˆ†æï¼ˆãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹é©ç”¨ï¼‰ ---\")\n",
    "        print(\"å“è©ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã€åŸå‹åŒ–ã€ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰é™¤å»ãªã©ã‚’è¡Œã„ã€æ„å‘³ã®ã‚ã‚‹å˜èªã‚’æŠ½å‡ºã—ã¾ã™ã€‚\")\n",
    "\n",
    "        node = tagger.parseToNode(text)\n",
    "        words = []\n",
    "        # ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ã®å®šç¾©ï¼ˆã‚«ã‚¿ã‚«ãƒŠã®åŸå‹ã‚‚è¿½åŠ ï¼‰\n",
    "        stop_words = ['ã“ã¨', 'ã‚‚ã®', 'ãŸã‚', 'ã“ã‚Œ', 'ãã‚Œ', 'ã‚ã‚Œ', 'ç§', 'ã‚ˆã†', 'ã•ã‚“', \n",
    "                      'ã™ã‚‹', 'ã„ã‚‹', 'ãªã‚‹', 'ã‚ã‚‹', 'ã„ã†', 'ã‚¹ãƒ«', 'ã‚¤ãƒ«']\n",
    "\n",
    "        while node:\n",
    "            # BOS/EOS (æ–‡é ­ãƒ»æ–‡æœ«) ã‚„ç©ºã®ãƒãƒ¼ãƒ‰ã¯ã‚¹ã‚­ãƒƒãƒ—\n",
    "            if node.surface == \"\":\n",
    "                node = node.next\n",
    "                continue\n",
    "\n",
    "            # å“è©æƒ…å ±ãªã©ã‚’ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã§å–å¾—\n",
    "            features = node.feature.split(',')\n",
    "            \n",
    "            # ã‚¨ãƒ©ãƒ¼å›é¿: featuresã®è¦ç´ æ•°ãŒè¶³ã‚Šãªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—\n",
    "            if len(features) < 7:\n",
    "                node = node.next\n",
    "                continue\n",
    "\n",
    "            pos = features[0]         # å“è©\n",
    "            pos_detail1 = features[1] # å“è©ç´°åˆ†é¡1\n",
    "            original_form = features[6] # åŸå‹\n",
    "            surface_form = node.surface # è¡¨å±¤å½¢\n",
    "\n",
    "            # æŠ½å‡ºå¯¾è±¡ã®å“è©ã‚’å®šç¾© (åè©ã€å‹•è©ã€å½¢å®¹è©ã€å‰¯è©)\n",
    "            target_pos = ['åè©', 'å‹•è©', 'å½¢å®¹è©', 'å‰¯è©']\n",
    "\n",
    "            if pos in target_pos:\n",
    "                # å“è©ã«ã‚ˆã£ã¦åŸå‹ã‚’ä½¿ã†ã‹è¡¨å±¤å½¢ã‚’ä½¿ã†ã‹é¸æŠ\n",
    "                if pos in ['å‹•è©', 'å½¢å®¹è©']:\n",
    "                    word_to_check = original_form\n",
    "                else: # åè©ã€å‰¯è©ãªã©\n",
    "                    word_to_check = surface_form\n",
    "\n",
    "                # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å‡¦ç†\n",
    "                is_valid = True\n",
    "                # 1. é™¤å¤–ã™ã‚‹å“è©ç´°åˆ†é¡ï¼ˆåè©ã®å ´åˆï¼‰\n",
    "                if pos == 'åè©' and pos_detail1 in ['éè‡ªç«‹', 'ä»£åè©', 'æ•°', 'æ¥å°¾', 'æ¥ç¶šè©çš„']:\n",
    "                    is_valid = False\n",
    "                # 2. å˜èªãŒ'*'ã‚„ç©ºã®å ´åˆã¯é™¤å¤–\n",
    "                if word_to_check == '*' or not word_to_check:\n",
    "                    is_valid = False\n",
    "                # 3. ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ã«å«ã¾ã‚Œã¦ã„ã‚Œã°é™¤å¤–\n",
    "                if word_to_check in stop_words:\n",
    "                    is_valid = False\n",
    "                # 4. 1æ–‡å­—ã®ã²ã‚‰ãŒãªãƒ»ã‚«ã‚¿ã‚«ãƒŠã¯é™¤å¤– (æ¼¢å­—ã‚„ã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆã¯æ®‹ã™)\n",
    "                if len(word_to_check) == 1 and re.fullmatch(r'[ã-ã‚“ã‚¡-ãƒ¶]', word_to_check):\n",
    "                    is_valid = False\n",
    "\n",
    "                if is_valid:\n",
    "                    words.append(word_to_check)\n",
    "            \n",
    "            node = node.next\n",
    "        \n",
    "        word_counts = Counter(words)\n",
    "        print(\"\\næœ€ã‚‚é »ç¹ã«å‡ºç¾ã™ã‚‹æ„å‘³ã®ã‚ã‚‹å˜èª (ãƒˆãƒƒãƒ—15):\")\n",
    "        if word_counts:\n",
    "            for word, count in word_counts.most_common(15):\n",
    "                print(f\"- {word}: {count}å›\")\n",
    "        else:\n",
    "            print(\"åˆ†æå¯¾è±¡ã®å˜èªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\")\n",
    "\n",
    "    except RuntimeError as e:\n",
    "        print(f\"MeCabã®å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "        print(\"MeCabã¾ãŸã¯è¾æ›¸ãŒæ­£ã—ãã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ãªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚\")\n",
    "        print(\"ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã®ã‚»ãƒ«ã§ `!pip install mecab-python3 ipadic` ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚\")\n",
    "    except Exception as e:\n",
    "        print(f\"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}\")\n",
    "\n",
    "# --- ãƒ¡ã‚¤ãƒ³å‡¦ç† ---\n",
    "\n",
    "print(\"æº–å‚™OK\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d62513-b024-4b25-bc64-273329642f70",
   "metadata": {},
   "source": [
    "`æº–å‚™OK` ã¨å‡ºåŠ›ã•ã‚ŒãŸã‚‰ã€ãƒ†ã‚­ã‚¹ãƒˆåˆ†æãŒã§ãã‚‹çŠ¶æ…‹ã«ãªã£ã¦ã„ã¾ã™ã€‚\n",
    "\n",
    "æ¬¡ã®ã‚»ãƒ«ã‚’å®Ÿè¡Œã—ã¦ã€ãƒ†ã‚­ã‚¹ãƒˆåˆ†æã‚’ä½“é¨“ã—ã¾ã—ã‚‡ã†ã€‚ã“ã¡ã‚‰ã‚‚åŒæ§˜ã«ã€ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³å½¢å¼ï¼‰ã‚’èª­ã¿è¾¼ã‚“ã§åˆ†æã™ã‚‹ã‚ˆã†ã«ãªã£ã¦ã„ã¾ã™ã®ã§ã€æ…£ã‚Œã¦ããŸã‚‰åˆ¥ã®ãƒ•ã‚¡ã‚¤ãƒ«ã§ã‚‚è©¦ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9fc6ac-2b82-4520-9ca6-7b1f1e8ce47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ†æå¯¾è±¡ã®ã‚µãƒ³ãƒ—ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«\n",
    "file_path = \"sample-text/sample.md\"\n",
    "\n",
    "\n",
    "\"\"\"ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆ†æ\"\"\"\n",
    "try:\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        sample_text = f.read()\n",
    "    \n",
    "    analyze_word_frequency_with_mecab(sample_text)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ã‚¨ãƒ©ãƒ¼: ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ‘ã‚¹ã‚’ç¢ºèªã—ã¦ãã ã•ã„: {file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"ãƒ¡ã‚¤ãƒ³å‡¦ç†ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cde6ab-32b3-4f5d-b7d7-9a098dac4eac",
   "metadata": {},
   "source": [
    "## doc2vec\n",
    "### doc2vec ã¨ã¯\n",
    "\n",
    "Doc2Vecã¯ä»»æ„ã®é•·ã•ã®æ–‡ç« ã‚’å›ºå®šé•·ã®ãƒ™ã‚¯ãƒˆãƒ«ã«å¤‰æ›ã™ã‚‹æŠ€è¡“ã§ã™ã€‚\n",
    "Word2VecãŒå˜èªã®åˆ†æ•£è¡¨ç¾ã‚’ç²å¾—ã™ã‚‹ã‚‚ã®ã ã£ãŸã®ã«å¯¾ã—ã€Doc2Vecã¯æ–‡ç« ã‚„æ–‡æ›¸ã®åˆ†æ•£è¡¨ç¾ã‚’ç²å¾—ã—ã¾ã™ã€‚æ–‡ç« ã®åˆ†æ•£è¡¨ç¾ã‚’ç²å¾—ã™ã‚‹æ‰‹æ³•ã¨ã—ã¦ã¯å¤å…¸çš„ãªã‚‚ã®ã¨ã—ã¦ã¯Bag-of-Wordsã‚„TF-IDFãŒã‚ã‚Šã¾ã™ãŒã€ãã‚Œã‚‰ã¯ä¸‹è¨˜ã®ã‚ˆã†ãªå¼±ç‚¹ã‚’æœ‰ã—ã¦ã„ã¾ã™ã€‚\n",
    "\n",
    "æ–‡ç« å†…ã®å˜èªã®èªé †æƒ…å ±ã‚’æœ‰ã—ã¦ã„ãªã„\n",
    "åŒç¾©èªã§ã‚‚å®Œå…¨ã«ç•°ãªã‚‹ç‹¬ç«‹ã—ãŸå˜èªã¨ã—ã¦èªè­˜ã™ã‚‹\n",
    "ã“ã‚Œã‚‰ã¯ã‚«ã‚¦ãƒ³ãƒˆãƒ™ãƒ¼ã‚¹ã¨å‘¼ã°ã‚Œã‚‹æ‰‹æ³•ã§ã™ãŒã€Doc2Vecã¯ä¸Šè¨˜å¼±ç‚¹ã‚’å…‹æœã™ã¹ãé•ã†ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§æ–‡ç« ã®åˆ†æ•£è¡¨ç¾ã®ç²å¾—ã‚’è©¦ã¿ã¦ã„ã¾ã™ã€‚\n",
    "\n",
    "> å¼•ç”¨: [(Qiita) Doc2Vecã«ã¤ã„ã¦ã¾ã¨ã‚ã‚‹](https://qiita.com/g-k/items/5ea94c13281f675302ca#doc2vec%E3%81%A8%E3%81%AF%E4%BD%95%E3%81%8B)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564d56a0-ed20-4594-868f-18e56cf47648",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import MeCab\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "from collections import Counter\n",
    "import json\n",
    "\n",
    "\n",
    "# --- è¨­å®š ---\n",
    "MECAB_ARGS = \"\"\n",
    "SAMPLE_DIR_PATH = \"sample-text/\"    # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®å…¨ã¦ã®æ–‡æ›¸ã‚’å¯¾è±¡ã«åˆ†æã—ã¾ã™\n",
    "\n",
    "# ãƒãƒ³ã‚ºã‚ªãƒ³å‘ã‘è¨­å®š\n",
    "MAX_FILES = 5  # é©åº¦ãªãƒ•ã‚¡ã‚¤ãƒ«æ•°\n",
    "MAX_TEXT_LENGTH = 3000  # ååˆ†ãªåˆ†æãƒ‡ãƒ¼ã‚¿\n",
    "EPOCHS = 15  # ãƒãƒ©ãƒ³ã‚¹ã®å–ã‚ŒãŸã‚¨ãƒãƒƒã‚¯æ•°\n",
    "MIN_COUNT = 2\n",
    "VECTOR_SIZE = 100\n",
    "\n",
    "\n",
    "class Doc2VecAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.documents = []\n",
    "        self.document_names = []\n",
    "        self.word_stats = {}\n",
    "        \n",
    "    def preprocess_text(self, text, doc_name):\n",
    "        \"\"\"\n",
    "        é«˜å“è³ªãªå‰å‡¦ç†ï¼šçµ±è¨ˆæƒ…å ±ã‚‚åé›†\n",
    "        \"\"\"\n",
    "        if len(text) > MAX_TEXT_LENGTH:\n",
    "            text = text[:MAX_TEXT_LENGTH]\n",
    "        \n",
    "        tagger = MeCab.Tagger(MECAB_ARGS)\n",
    "        tagger.parse('')\n",
    "        node = tagger.parseToNode(text)\n",
    "        \n",
    "        words = []\n",
    "        pos_stats = Counter()\n",
    "        \n",
    "        # æ‹¡å¼µã•ã‚ŒãŸstop wordsãƒªã‚¹ãƒˆ\n",
    "        stop_words = {\n",
    "            # åŸºæœ¬çš„ãªæ©Ÿèƒ½èª\n",
    "            'ã“ã¨', 'ã‚‚ã®', 'ãŸã‚', 'ã“ã‚Œ', 'ãã‚Œ', 'ã‚ã‚Œ', 'ã©ã‚Œ', 'ã‚ˆã†', 'ã•ã‚“', 'ã¨ã“ã‚',\n",
    "            'ã¨ã', 'ã¨ã“', 'ãªã©', 'ãªã«', 'ãªã‚“', 'ã©ã“', 'ã„ã¤', 'ã ã‚Œ', 'ã©ã†', 'ãªãœ',\n",
    "            \n",
    "            # ä»£åè©ãƒ»æŒ‡ç¤ºèª\n",
    "            'ç§', 'åƒ•', 'ä¿º', 'å›', 'å½¼', 'å½¼å¥³', 'ã‚ãªãŸ', 'ã¿ãªã•ã‚“', 'ã¿ã‚“ãª',\n",
    "            'ã“ã“', 'ãã“', 'ã‚ãã“', 'ã©ã“ã‹', 'ã„ã¾', 'ã„ã¤ã‹', 'ã©ã“ã§ã‚‚',\n",
    "            \n",
    "            # åŠ©è©çš„ãªåè©\n",
    "            'ä¸Š', 'ä¸‹', 'ä¸­', 'å‰', 'å¾Œ', 'å·¦', 'å³', 'æ¨ª', 'éš£', 'é–“', 'å†…', 'å¤–',\n",
    "            'å…ˆ', 'å¥¥', 'æ‰‹å‰', 'å‘ã“ã†', 'ä»¥ä¸Š', 'ä»¥ä¸‹', 'æœªæº€', 'ç¨‹åº¦', 'ä»¥å¤–',\n",
    "            \n",
    "            # æ™‚é–“ãƒ»é »åº¦è¡¨ç¾\n",
    "            'ä»Šæ—¥', 'æ˜¨æ—¥', 'æ˜æ—¥', 'ä»Šå¹´', 'å»å¹´', 'æ¥å¹´', 'ä»Šæœˆ', 'å…ˆæœˆ', 'æ¥æœˆ',\n",
    "            'æ¯æ—¥', 'æ¯å›', 'æ¯å¹´', 'æ¯æœˆ', 'æ¯é€±', 'å¸¸ã«', 'ã„ã¤ã‚‚', 'ãŸã¾ã«',\n",
    "            \n",
    "            # æ•°é‡ãƒ»ç¨‹åº¦è¡¨ç¾\n",
    "            'å…¨ã¦', 'å…¨éƒ¨', 'ã™ã¹ã¦', 'ä¸€éƒ¨', 'åŠåˆ†', 'å¤§éƒ¨åˆ†', 'å°‘ã—', 'ã¡ã‚‡ã£ã¨',\n",
    "            'ã‹ãªã‚Š', 'ã¨ã¦ã‚‚', 'ã™ã”ã', 'ã‚ã¡ã‚ƒãã¡ã‚ƒ', 'éå¸¸', 'æ¥µã‚ã¦',\n",
    "            \n",
    "            # æ¥ç¶šãƒ»è»¢æ›è¡¨ç¾\n",
    "            'ã—ã‹ã—', 'ã§ã‚‚', 'ã ãŒ', 'ãŸã ã—', 'ã¨ã“ã‚ãŒ', 'ã‘ã‚Œã©', 'ã‘ã‚Œã©ã‚‚',\n",
    "            'ãã—ã¦', 'ã¾ãŸ', 'ã•ã‚‰ã«', 'ãã‚Œã‹ã‚‰', 'ãã‚Œã§', 'ãã“ã§', 'ã¤ã¾ã‚Š',\n",
    "            \n",
    "            # æ„Ÿå˜†ãƒ»å¿œç­”è¡¨ç¾\n",
    "            'ã¯ã„', 'ã„ã„ãˆ', 'ãˆãˆ', 'ã†ã‚“', 'ãã†', 'ãã†ã§ã™', 'ãªã‚‹ã»ã©',\n",
    "            'ãŠãŠ', 'ã‚ã‚', 'ã†ãƒ¼ã‚“', 'ãˆãƒ¼', 'ã¾ã‚', 'ã‚„ã¯ã‚Š', 'ã‚„ã£ã±ã‚Š',\n",
    "            \n",
    "            # ä¸€èˆ¬çš„ã™ãã‚‹å‹•è©ãƒ»å½¢å®¹è©ã®èªå¹¹\n",
    "            'ã™ã‚‹', 'ãªã‚‹', 'ã‚ã‚‹', 'ã„ã‚‹', 'ã§ãã‚‹', 'ã¿ã‚‹', 'ã„ã', 'ãã‚‹',\n",
    "            'ã„ã„', 'ã‚ˆã„', 'æ‚ªã„', 'å¤§ãã„', 'å°ã•ã„', 'æ–°ã—ã„', 'å¤ã„',\n",
    "            \n",
    "            # ãƒ“ã‚¸ãƒã‚¹ãƒ»æŠ€è¡“æ–‡æ›¸ã§ã‚ˆãå‡ºã‚‹ä¸€èˆ¬èª\n",
    "            'å ´åˆ', 'çŠ¶æ³', 'çŠ¶æ…‹', 'æ–¹æ³•', 'æ‰‹æ®µ', 'æ–¹å¼', 'å½¢å¼', 'ç¨®é¡', 'æ–¹å‘',\n",
    "            'çµæœ', 'åŠ¹æœ', 'å½±éŸ¿', 'é–¢ä¿‚', 'é–¢é€£', 'å¯¾è±¡', 'ç›®çš„', 'ç†ç”±', 'åŸå› ',\n",
    "            'å•é¡Œ', 'èª²é¡Œ', 'è§£æ±º', 'æ”¹å–„', 'å‘ä¸Š', 'ç™ºå±•', 'é€²æ­©', 'å¤‰åŒ–', 'å¤‰æ›´',\n",
    "            \n",
    "            # å˜ä½ãƒ»åŠ©æ•°è©çš„è¡¨ç¾\n",
    "            'å€‹', 'æœ¬', 'æš', 'å°', 'äºº', 'å›', 'åº¦', 'å€', 'å‰²', 'ãƒ‘ãƒ¼ã‚»ãƒ³ãƒˆ',\n",
    "            'æ™‚é–“', 'åˆ†', 'ç§’', 'æ—¥', 'é€±é–“', 'ãƒ¶æœˆ', 'å¹´é–“', 'ãƒ¡ãƒ¼ãƒˆãƒ«', 'ã‚­ãƒ­'\n",
    "        }\n",
    "\n",
    "        while node:\n",
    "            if not node.surface:\n",
    "                node = node.next\n",
    "                continue\n",
    "\n",
    "            features = node.feature.split(',')\n",
    "            if len(features) < 7:\n",
    "                node = node.next\n",
    "                continue\n",
    "\n",
    "            pos = features[0]\n",
    "            pos_detail1 = features[1]\n",
    "            original_form = features[6]\n",
    "            surface_form = node.surface\n",
    "\n",
    "            # çµ±è¨ˆåé›†\n",
    "            pos_stats[pos] += 1\n",
    "\n",
    "            # é«˜å“è³ªãªå“è©é¸æŠ\n",
    "            target_pos = ['åè©', 'å‹•è©', 'å½¢å®¹è©']\n",
    "            if pos in target_pos:\n",
    "                word_to_check = original_form if pos in ['å‹•è©', 'å½¢å®¹è©'] else surface_form\n",
    "                \n",
    "                # é«˜å“è³ªãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°\n",
    "                if (word_to_check != '*' and \n",
    "                    word_to_check not in stop_words and \n",
    "                    len(word_to_check) > 1 and\n",
    "                    not re.match(r'^[0-9]+$', word_to_check) and  # æ•°å­—ã®ã¿é™¤å¤–\n",
    "                    pos != 'åè©' or pos_detail1 not in ['æ•°', 'éè‡ªç«‹', 'ä»£åè©']):\n",
    "                    words.append(word_to_check)\n",
    "\n",
    "            node = node.next\n",
    "        \n",
    "        # æ–‡æ›¸çµ±è¨ˆã‚’ä¿å­˜\n",
    "        self.word_stats[doc_name] = {\n",
    "            'total_words': len(words),\n",
    "            'unique_words': len(set(words)),\n",
    "            'pos_distribution': dict(pos_stats.most_common(5))\n",
    "        }\n",
    "        \n",
    "        return words\n",
    "\n",
    "    def train_model(self, documents, document_names):\n",
    "        \"\"\"\n",
    "        Doc2Vecãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’\n",
    "        \"\"\"\n",
    "        print(\"=== Doc2Vec ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ãƒ•ã‚§ãƒ¼ã‚º ===\")\n",
    "        \n",
    "        tagged_documents = [\n",
    "            TaggedDocument(doc, [name]) \n",
    "            for doc, name in zip(documents, document_names)\n",
    "        ]\n",
    "        \n",
    "        self.model = Doc2Vec(\n",
    "            vector_size=VECTOR_SIZE,\n",
    "            min_count=MIN_COUNT,\n",
    "            epochs=EPOCHS,\n",
    "            workers=min(4, os.cpu_count()),\n",
    "            dm=1,  # PV-DM (åˆ†æ•£ãƒ¡ãƒ¢ãƒª)\n",
    "            window=5,\n",
    "            alpha=0.025,\n",
    "            min_alpha=0.00025\n",
    "        )\n",
    "        \n",
    "        start_time = time.time()\n",
    "        self.model.build_vocab(tagged_documents)\n",
    "        \n",
    "        print(f\"å­¦ç¿’é–‹å§‹: {len(documents)}æ–‡æ›¸, èªå½™æ•°: {len(self.model.wv.key_to_index)}\")\n",
    "        self.model.train(tagged_documents, total_examples=self.model.corpus_count, epochs=self.model.epochs)\n",
    "        \n",
    "        training_time = time.time() - start_time\n",
    "        print(f\"å­¦ç¿’å®Œäº† (æ‰€è¦æ™‚é–“: {training_time:.2f}ç§’)\")\n",
    "        \n",
    "        return self.model\n",
    "\n",
    "    def analyze_document_similarity(self):\n",
    "        \"\"\"\n",
    "        æ–‡æ›¸é–“é¡ä¼¼åº¦åˆ†æ\n",
    "        \"\"\"\n",
    "        print(\"\\n=== æ–‡æ›¸é–“é¡ä¼¼åº¦åˆ†æ ===\")\n",
    "        \n",
    "        if len(self.document_names) < 2:\n",
    "            print(\"é¡ä¼¼åº¦åˆ†æã«ã¯2ã¤ä»¥ä¸Šã®æ–‡æ›¸ãŒå¿…è¦ã§ã™ã€‚\")\n",
    "            return\n",
    "        \n",
    "        similarities = []\n",
    "        for i, doc1 in enumerate(self.document_names):\n",
    "            for j, doc2 in enumerate(self.document_names[i+1:], i+1):\n",
    "                try:\n",
    "                    similarity = self.model.dv.similarity(doc1, doc2)\n",
    "                    similarities.append((doc1, doc2, similarity))\n",
    "                    print(f\"{os.path.basename(doc1)} âŸ· {os.path.basename(doc2)}: {similarity:.4f}\")\n",
    "                except KeyError as e:\n",
    "                    print(f\"æ–‡æ›¸ãƒ™ã‚¯ãƒˆãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {e}\")\n",
    "        \n",
    "        # æœ€ã‚‚é¡ä¼¼ã—ãŸæ–‡æ›¸ãƒšã‚¢\n",
    "        if similarities:\n",
    "            most_similar = max(similarities, key=lambda x: x[2])\n",
    "            print(f\"\\næœ€ã‚‚é¡ä¼¼ã—ãŸæ–‡æ›¸ãƒšã‚¢:\")\n",
    "            print(f\"  {os.path.basename(most_similar[0])} âŸ· {os.path.basename(most_similar[1])}\")\n",
    "            print(f\"  é¡ä¼¼åº¦: {most_similar[2]:.4f}\")\n",
    "\n",
    "    def analyze_word_similarity(self):\n",
    "        \"\"\"\n",
    "        å˜èªé¡ä¼¼åº¦åˆ†æï¼ˆæ•™è‚²çš„ãªè§£èª¬ä»˜ãï¼‰\n",
    "        \"\"\"\n",
    "        print(\"\\n=== å˜èªé¡ä¼¼åº¦åˆ†æ ===\")\n",
    "        \n",
    "        # èªå½™ã‹ã‚‰é »å‡ºå˜èªã‚’é¸æŠ\n",
    "        word_freq = Counter()\n",
    "        for doc in self.documents:\n",
    "            word_freq.update(doc)\n",
    "        \n",
    "        common_words = [word for word, freq in word_freq.most_common(20) \n",
    "                       if word in self.model.wv.key_to_index]\n",
    "        \n",
    "        if not common_words:\n",
    "            print(\"åˆ†æå¯èƒ½ãªå˜èªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚\")\n",
    "            return\n",
    "        \n",
    "        print(f\"é »å‡ºå˜èªã‹ã‚‰åˆ†æå¯¾è±¡ã‚’é¸æŠ: {common_words[:5]}\")\n",
    "        \n",
    "        for word in common_words[:3]:\n",
    "            print(f\"\\n'{word}' ã®é¡ä¼¼å˜èª:\")\n",
    "            try:\n",
    "                similar_words = self.model.wv.most_similar(word, topn=5)\n",
    "                for i, (sim_word, similarity) in enumerate(similar_words, 1):\n",
    "                    print(f\"  {i}. {sim_word} (é¡ä¼¼åº¦: {similarity:.4f})\")\n",
    "            except KeyError:\n",
    "                print(f\"  '{word}' ã®é¡ä¼¼å˜èªã‚’è¦‹ã¤ã‘ã‚‰ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚\")\n",
    "\n",
    "    def analyze_word_clusters(self):\n",
    "        \"\"\"\n",
    "        å˜èªã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°åˆ†æ\n",
    "        \"\"\"\n",
    "        print(\"\\n=== å˜èªã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°åˆ†æ ===\")\n",
    "        \n",
    "        # é »å‡ºå˜èªã®ãƒ™ã‚¯ãƒˆãƒ«ã‚’å–å¾—\n",
    "        word_freq = Counter()\n",
    "        for doc in self.documents:\n",
    "            word_freq.update(doc)\n",
    "        \n",
    "        target_words = [word for word, freq in word_freq.most_common(15) \n",
    "                       if word in self.model.wv.key_to_index]\n",
    "        \n",
    "        if len(target_words) < 3:\n",
    "            print(\"ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã«ååˆ†ãªå˜èªãŒã‚ã‚Šã¾ã›ã‚“ã€‚\")\n",
    "            return\n",
    "        \n",
    "        print(f\"åˆ†æå¯¾è±¡å˜èª: {target_words}\")\n",
    "        \n",
    "        # ç°¡æ˜“ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ï¼ˆé¡ä¼¼åº¦ãƒ™ãƒ¼ã‚¹ï¼‰\n",
    "        clusters = {}\n",
    "        processed = set()\n",
    "        \n",
    "        for word in target_words:\n",
    "            if word in processed:\n",
    "                continue\n",
    "                \n",
    "            cluster = [word]\n",
    "            processed.add(word)\n",
    "            \n",
    "            try:\n",
    "                similar_words = self.model.wv.most_similar(word, topn=10)\n",
    "                for sim_word, similarity in similar_words:\n",
    "                    if sim_word in target_words and similarity > 0.3 and sim_word not in processed:\n",
    "                        cluster.append(sim_word)\n",
    "                        processed.add(sim_word)\n",
    "                \n",
    "                if len(cluster) > 1:\n",
    "                    clusters[f\"ã‚¯ãƒ©ã‚¹ã‚¿_{len(clusters)+1}\"] = cluster\n",
    "            except KeyError:\n",
    "                continue\n",
    "        \n",
    "        print(\"\\nå˜èªã‚¯ãƒ©ã‚¹ã‚¿:\")\n",
    "        for cluster_name, words in clusters.items():\n",
    "            print(f\"  {cluster_name}: {', '.join(words)}\")\n",
    "\n",
    "    def generate_analysis_report(self):\n",
    "        \"\"\"\n",
    "        åˆ†æãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"=== Doc2Vec åˆ†æãƒ¬ãƒãƒ¼ãƒˆ ===\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        print(f\"\\nã€ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã€‘\")\n",
    "        print(f\"  ãƒ™ã‚¯ãƒˆãƒ«æ¬¡å…ƒæ•°: {self.model.vector_size}\")\n",
    "        print(f\"  èªå½™æ•°: {len(self.model.wv.key_to_index)}\")\n",
    "        print(f\"  å­¦ç¿’ã‚¨ãƒãƒƒã‚¯æ•°: {self.model.epochs}\")\n",
    "        \n",
    "        print(f\"\\nã€æ–‡æ›¸çµ±è¨ˆã€‘\")\n",
    "        for doc_name, stats in self.word_stats.items():\n",
    "            print(f\"  {os.path.basename(doc_name)}:\")\n",
    "            print(f\"    ç·å˜èªæ•°: {stats['total_words']}\")\n",
    "            print(f\"    ãƒ¦ãƒ‹ãƒ¼ã‚¯å˜èªæ•°: {stats['unique_words']}\")\n",
    "            print(f\"    èªå½™å¤šæ§˜æ€§: {stats['unique_words']/stats['total_words']:.3f}\")\n",
    "        \n",
    "        print(f\"\\nã€é »å‡ºå˜èªãƒˆãƒƒãƒ—10ã€‘\")\n",
    "        word_freq = Counter()\n",
    "        for doc in self.documents:\n",
    "            word_freq.update(doc)\n",
    "        \n",
    "        for i, (word, freq) in enumerate(word_freq.most_common(10), 1):\n",
    "            print(f\"  {i:2d}. {word} ({freq}å›)\")\n",
    "\n",
    "\n",
    "print(\"æº–å‚™OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16cc468-293d-41ac-9e0f-d804a6c7590e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    ãƒ¡ã‚¤ãƒ³å‡¦ç†\n",
    "    \"\"\"\n",
    "    analyzer = Doc2VecAnalyzer()\n",
    "\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "\n",
    "        # ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿\n",
    "        search_path = os.path.join(SAMPLE_DIR_PATH, '*.md')\n",
    "        file_paths = glob.glob(search_path)\n",
    "\n",
    "        if not file_paths:\n",
    "            print(f\"ã‚¨ãƒ©ãƒ¼: ã‚µãƒ³ãƒ—ãƒ«æ–‡æ›¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {SAMPLE_DIR_PATH}\")\n",
    "            return\n",
    "\n",
    "        # ãƒ•ã‚¡ã‚¤ãƒ«æ•°åˆ¶é™\n",
    "        limited_files = file_paths[:MAX_FILES]\n",
    "        print(f\"=== ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ ===\")\n",
    "        print(f\"å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(limited_files)}\")\n",
    "\n",
    "        documents = []\n",
    "        document_names = []\n",
    "\n",
    "        for i, path in enumerate(limited_files):\n",
    "            filename = os.path.basename(path)\n",
    "            print(f\"å‡¦ç†ä¸­: {filename} ({i+1}/{len(limited_files)})\")\n",
    "\n",
    "            with open(path, 'r', encoding='utf-8') as f:\n",
    "                text = f.read()\n",
    "\n",
    "            words = analyzer.preprocess_text(text, path)\n",
    "            if words:\n",
    "                documents.append(words)\n",
    "                document_names.append(path)\n",
    "                print(f\"  æŠ½å‡ºå˜èªæ•°: {len(words)}\")\n",
    "\n",
    "        if not documents:\n",
    "            print(\"ã‚¨ãƒ©ãƒ¼: åˆ†æå¯èƒ½ãªæ–‡æ›¸ãŒã‚ã‚Šã¾ã›ã‚“ã€‚\")\n",
    "            return\n",
    "\n",
    "        analyzer.documents = documents\n",
    "        analyzer.document_names = document_names\n",
    "\n",
    "        # ãƒ¢ãƒ‡ãƒ«å­¦ç¿’\n",
    "        analyzer.train_model(documents, document_names)\n",
    "\n",
    "        # å„ç¨®åˆ†æå®Ÿè¡Œ\n",
    "        analyzer.analyze_document_similarity()\n",
    "        analyzer.analyze_word_similarity()\n",
    "        analyzer.analyze_word_clusters()\n",
    "        analyzer.generate_analysis_report()\n",
    "\n",
    "        total_time = time.time() - start_time\n",
    "        print(f\"\\nç·å‡¦ç†æ™‚é–“: {total_time:.2f}ç§’\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8a98cf-fdcb-49b4-93af-c5e3faa362d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b071575-2863-4b98-b38c-be2b5bf9660f",
   "metadata": {},
   "source": [
    "# ç”ŸæˆAIã‚’ä½¿ç”¨ã™ã‚‹å ´åˆ\n",
    "ã“ã“ã‹ã‚‰ã¯ã€ç”ŸæˆAIã‚’ä½¿ç”¨ã—ãŸè‡ªç„¶è¨€èªå‡¦ç†ã®ã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ¼ãƒ‰ã§ã™ã€‚\n",
    "\n",
    "[Amazon Bedrock](https://aws.amazon.com/jp/bedrock/) ã‚’ä½¿ç”¨ã™ã‚‹ã¨ã€æ§˜ã€…ãªç”ŸæˆAIãƒ¢ãƒ‡ãƒ«ã‚’åˆ©ç”¨ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ä»Šå›ã¯ã€ `Anthropic` ãŒæä¾›ã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆç³»ã® LLM ã§ã‚ã‚‹ `Claude 4 Sonnet` ã‚’åˆ©ç”¨ã—ã¦ã€ãƒ†ã‚­ã‚¹ãƒˆåˆ†æã‚’å®Ÿæ–½ã—ã¾ã™ã€‚\n",
    "\n",
    "åˆ†æã®ã‚¹ãƒ†ãƒƒãƒ—ã¯ä»¥ä¸‹ã®ã¨ãŠã‚Šã§ã™ã€‚\n",
    "\n",
    "1. LLM ã§ç‰¹å¾´çš„ãªå˜èªã‚’æŠ½å‡ºã—ã¦ãƒªã‚¹ãƒˆã¨ã—ã¦å–å¾—ã™ã‚‹\n",
    "2. LLM ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã«ä¸è¦ãªãƒ†ã‚­ã‚¹ãƒˆãŒå«ã¾ã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã®ã§ã€å¿…è¦ãªãƒ¬ã‚¹ãƒãƒ³ã‚¹(json)ã ã‘ã‚’ãƒ‘ãƒ¼ã‚¹ã™ã‚‹ã€‚\n",
    "3. ãƒªã‚¹ãƒˆã®ç‰¹å¾´å¾Œã‚’1ã¤ãšã¤ã€å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆå†…ã§ã‚«ã‚¦ãƒ³ãƒˆã™ã‚‹ã€‚ >>> çµæœ: `analysis_result.json` \n",
    "4. (ãŠã¾ã‘) å‡ºç¾å›æ•°ãŒå¤šã„ãƒ¯ãƒ¼ãƒ‰ã‚’ã€ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã§å¯è¦–åŒ–ã™ã‚‹ >>> çµæœ: `wordcloud_result.png`\n",
    "\n",
    "(å‚è€ƒ)\n",
    "LLM ç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¯ `templete/llm.md` ã«ã‚ã‚Šã¾ã™ã€‚ GenU ã®ãƒ“ãƒ«ãƒ€ãƒ¼ãƒ¢ãƒ¼ãƒ‰ã§ã‚‚åˆ©ç”¨ã§ãã‚‹ã‚ˆã†ã«ä½œæˆã•ã‚Œã¦ã„ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2e24cd-2d74-4ccb-b2d9-8971a20bbaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import re\n",
    "from datetime import datetime\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def extract_json_from_text(text):\n",
    "    \"\"\"\n",
    "    ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰JSONã‚’æŠ½å‡ºã™ã‚‹é–¢æ•°\n",
    "    LLMã®å‡ºåŠ›ã«JSONä»¥å¤–ã®ãƒ†ã‚­ã‚¹ãƒˆãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã«å¯¾å¿œ\n",
    "    \"\"\"\n",
    "    # è¤‡æ•°ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã§JSONã‚’æ¢ã™\n",
    "    patterns = [\n",
    "        # ãƒ‘ã‚¿ãƒ¼ãƒ³1: æœ€åˆã®{ã‹ã‚‰æœ€å¾Œã®}ã¾ã§\n",
    "        r'\\{.*\\}',\n",
    "        # ãƒ‘ã‚¿ãƒ¼ãƒ³2: ```json ãƒ–ãƒ­ãƒƒã‚¯å†…\n",
    "        r'```json\\s*(\\{.*?\\})\\s*```',\n",
    "        # ãƒ‘ã‚¿ãƒ¼ãƒ³3: ``` ãƒ–ãƒ­ãƒƒã‚¯å†…\n",
    "        r'```\\s*(\\{.*?\\})\\s*```'\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        matches = re.findall(pattern, text, re.DOTALL)\n",
    "        for match in matches:\n",
    "            try:\n",
    "                # JSONã¨ã—ã¦è§£æã‚’è©¦è¡Œ\n",
    "                parsed = json.loads(match)\n",
    "                return parsed\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "    \n",
    "    # è¡Œã”ã¨ã«åˆ†å‰²ã—ã¦JSONã‚‰ã—ã„è¡Œã‚’æ¢ã™\n",
    "    lines = text.split('\\n')\n",
    "    json_lines = []\n",
    "    in_json = False\n",
    "    brace_count = 0\n",
    "    \n",
    "    for line in lines:\n",
    "        stripped = line.strip()\n",
    "        if stripped.startswith('{'):\n",
    "            in_json = True\n",
    "            json_lines = [line]\n",
    "            brace_count = line.count('{') - line.count('}')\n",
    "        elif in_json:\n",
    "            json_lines.append(line)\n",
    "            brace_count += line.count('{') - line.count('}')\n",
    "            if brace_count <= 0:\n",
    "                break\n",
    "    \n",
    "    if json_lines:\n",
    "        try:\n",
    "            json_text = '\\n'.join(json_lines)\n",
    "            return json.loads(json_text)\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "    \n",
    "    return None\n",
    "\n",
    "def create_wordcloud(word_frequency, top_n=10):\n",
    "    \"\"\"\n",
    "    å˜èªã®å‡ºç¾å›æ•°ã‹ã‚‰ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã‚’ç”Ÿæˆã™ã‚‹é–¢æ•°\n",
    "    \"\"\"\n",
    "    # ä¸Šä½Nä½ã¾ã§ã®å˜èªã‚’å–å¾—\n",
    "    top_words = dict(list(word_frequency.items())[:top_n])\n",
    "    \n",
    "    if not top_words:\n",
    "        print(\"âŒ ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ç”¨ã®ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“\")\n",
    "        return\n",
    "    \n",
    "    print(f\"ğŸ¨ ä¸Šä½{top_n}ä½ã¾ã§ã®å˜èªã§ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã‚’ç”Ÿæˆä¸­...\")\n",
    "    \n",
    "    try:\n",
    "        # æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’æŒ‡å®š\n",
    "        font_path = \"./MEIRYO.TTC\"\n",
    "        \n",
    "        # ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã‚’ç”Ÿæˆ\n",
    "        wordcloud = WordCloud(\n",
    "            width=800,\n",
    "            height=400,\n",
    "            background_color='white',\n",
    "            max_words=top_n,\n",
    "            relative_scaling=0.5,\n",
    "            colormap='viridis',\n",
    "            font_path=font_path  # MEIRYOãƒ•ã‚©ãƒ³ãƒˆã‚’æŒ‡å®š\n",
    "        ).generate_from_frequencies(top_words)\n",
    "        \n",
    "        # ãƒ—ãƒ­ãƒƒãƒˆè¨­å®šï¼ˆæ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆå¯¾å¿œï¼‰\n",
    "        plt.rcParams['font.family'] = 'DejaVu Sans'  # è‹±èªéƒ¨åˆ†ç”¨\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.axis('off')\n",
    "        plt.title(f'Word Frequency Ranking Top {top_n}', fontsize=16, pad=20)\n",
    "        \n",
    "        # ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜\n",
    "        output_file = \"wordcloud_result.png\"\n",
    "        plt.savefig(output_file, bbox_inches='tight', dpi=300)\n",
    "        print(f\"âœ… ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã‚’ {output_file} ã«ä¿å­˜ã—ã¾ã—ãŸ\")\n",
    "        \n",
    "        # ç”»é¢ã«è¡¨ç¤º\n",
    "        plt.show()\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(\"âŒ MEIRYO.TTCãƒ•ã‚©ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“\")\n",
    "        print(\"ğŸ’¡ ã‚«ãƒ¬ãƒ³ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«MEIRYO.TTCãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ç”Ÿæˆã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}\")\n",
    "\n",
    "\n",
    "print(\"æº–å‚™OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5722a8-17bb-458a-a93a-33b32802658a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_text_with_bedrock():\n",
    "    \"\"\"\n",
    "    ãƒ¡ã‚¤ãƒ³åˆ†æé–¢æ•°ï¼šãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€Bedrockã§åˆ†æã—ã€çµæœã‚’å‡ºåŠ›\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. è¨­å®š\n",
    "    # Claude 4 Sonnetç”¨ã®æ¨è«–ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«IDã‚’ä½¿ç”¨\n",
    "    MODEL_ID = \"us.anthropic.claude-sonnet-4-20250514-v1:0\"\n",
    "    REGION = \"us-east-1\"\n",
    "    \n",
    "    print(\"ğŸš€ Amazon Bedrock ãƒ†ã‚­ã‚¹ãƒˆåˆ†æã‚’é–‹å§‹ã—ã¾ã™...\")\n",
    "    \n",
    "    # 2. ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿\n",
    "    try:\n",
    "        with open(\"sample-text/sample.md\", \"r\", encoding=\"utf-8\") as file:\n",
    "            sample_text = file.read()\n",
    "        print(\"âœ… ã‚µãƒ³ãƒ—ãƒ«ãƒ†ã‚­ã‚¹ãƒˆã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"âŒ sample.mdãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“\")\n",
    "        return\n",
    "    \n",
    "    # 3. ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’èª­ã¿è¾¼ã¿\n",
    "    try:\n",
    "        with open(\"templete/llm.md\", \"r\", encoding=\"utf-8\") as file:\n",
    "            prompt_template = file.read()\n",
    "        print(\"âœ… ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"âŒ llm.mdãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“\")\n",
    "        return\n",
    "    \n",
    "    # 4. Bedrockã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½œæˆ\n",
    "    bedrock_runtime = boto3.client(\n",
    "        service_name=\"bedrock-runtime\",\n",
    "        region_name=REGION\n",
    "    )\n",
    "    print(\"âœ… Bedrockã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’åˆæœŸåŒ–ã—ã¾ã—ãŸ\")\n",
    "    \n",
    "    # 5. ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆï¼ˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã«ãƒ†ã‚­ã‚¹ãƒˆã‚’åŸ‹ã‚è¾¼ã¿ï¼‰\n",
    "    prompt = prompt_template.replace(\"{{è§£æå¯¾è±¡ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’LLMã«æ¸¡ã™}}\", sample_text)\n",
    "    \n",
    "    # 6. Bedrock converse_stream APIã‚’å‘¼ã³å‡ºã—\n",
    "    try:\n",
    "        print(\"ğŸ¤– Claude 4 Sonnet ã§åˆ†æä¸­...\")\n",
    "        \n",
    "        response = bedrock_runtime.converse_stream(\n",
    "            modelId=MODEL_ID,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            \"text\": prompt\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            ],\n",
    "            inferenceConfig={\n",
    "                \"maxTokens\": 2000,\n",
    "                \"temperature\": 0.1\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’åé›†\n",
    "        llm_output = \"\"\n",
    "        for event in response[\"stream\"]:\n",
    "            if \"contentBlockDelta\" in event:\n",
    "                delta = event[\"contentBlockDelta\"][\"delta\"]\n",
    "                if \"text\" in delta:\n",
    "                    llm_output += delta[\"text\"]\n",
    "        \n",
    "        print(\"âœ… LLMã‹ã‚‰ã®å¿œç­”ã‚’å–å¾—ã—ã¾ã—ãŸ\")\n",
    "        print(f\"ğŸ“„ LLMå‡ºåŠ›ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼: {llm_output[:200]}...\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Bedrock APIå‘¼ã³å‡ºã—ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}\")\n",
    "        return\n",
    "    \n",
    "    # 7. LLMã®å‡ºåŠ›ã‹ã‚‰JSONã‚’æŠ½å‡ºï¼ˆå …ç‰¢ãªãƒ‘ãƒ¼ã‚¹å‡¦ç†ï¼‰\n",
    "    try:\n",
    "        print(\"ğŸ” JSONå½¢å¼ã®ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºä¸­...\")\n",
    "        \n",
    "        # å …ç‰¢ãªJSONæŠ½å‡ºã‚’å®Ÿè¡Œ\n",
    "        keywords_data = extract_json_from_text(llm_output)\n",
    "        \n",
    "        if keywords_data is None:\n",
    "            print(\"âŒ æœ‰åŠ¹ãªJSONãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ\")\n",
    "            print(f\"LLMå®Œå…¨å‡ºåŠ›:\\n{llm_output}\")\n",
    "            return\n",
    "        \n",
    "        # resultsã‚­ãƒ¼ã‹ã‚‰å˜èªé…åˆ—ã‚’å–å¾—\n",
    "        if \"results\" not in keywords_data:\n",
    "            print(\"âŒ 'results'ã‚­ãƒ¼ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“\")\n",
    "            print(f\"å–å¾—ã—ãŸJSON: {keywords_data}\")\n",
    "            return\n",
    "            \n",
    "        keywords = keywords_data[\"results\"]\n",
    "        print(f\"âœ… {len(keywords)}å€‹ã®ç‰¹å¾´çš„ãªå˜èªã‚’æŠ½å‡ºã—ã¾ã—ãŸ\")\n",
    "        print(f\"ğŸ“ æŠ½å‡ºã•ã‚ŒãŸå˜èª: {keywords}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ JSONè§£æã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}\")\n",
    "        print(f\"LLMå®Œå…¨å‡ºåŠ›:\\n{llm_output}\")\n",
    "        return\n",
    "    \n",
    "    # 8. å„å˜èªã®å‡ºç¾å›æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ\n",
    "    print(\"ğŸ” å˜èªã®å‡ºç¾å›æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆä¸­...\")\n",
    "    \n",
    "    word_count = {}\n",
    "    for keyword in keywords:\n",
    "        # æ­£è¦è¡¨ç¾ã§å˜èªã®å‡ºç¾å›æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆï¼ˆå¤§æ–‡å­—å°æ–‡å­—ã‚’åŒºåˆ¥ã—ãªã„ï¼‰\n",
    "        pattern = re.escape(keyword)\n",
    "        matches = re.findall(pattern, sample_text, re.IGNORECASE)\n",
    "        count = len(matches)\n",
    "        word_count[keyword] = count\n",
    "        print(f\"  ğŸ“ '{keyword}': {count}å›\")\n",
    "    \n",
    "    # 9. å‡ºç¾å›æ•°ã®å¤šã„é †ã«ä¸¦ã³æ›¿ãˆ\n",
    "    print(\"ğŸ“Š å‡ºç¾å›æ•°ã®å¤šã„é †ã«ä¸¦ã³æ›¿ãˆä¸­...\")\n",
    "    sorted_word_count = dict(sorted(word_count.items(), key=lambda x: x[1], reverse=True))\n",
    "    \n",
    "    print(\"ğŸ† å‡ºç¾å›æ•°ãƒ©ãƒ³ã‚­ãƒ³ã‚°:\")\n",
    "    for i, (word, count) in enumerate(sorted_word_count.items(), 1):\n",
    "        print(f\"  {i}ä½: '{word}' - {count}å›\")\n",
    "    \n",
    "    # 10. çµæœã‚’JSONå½¢å¼ã§å‡ºåŠ›\n",
    "    result = {\n",
    "        \"analysis_timestamp\": datetime.now().isoformat(),\n",
    "        \"source_file\": \"sample-text/sample.md\",\n",
    "        \"model_used\": MODEL_ID,\n",
    "        \"extracted_keywords\": keywords,\n",
    "        \"word_frequency\": sorted_word_count,  # ä¸¦ã³æ›¿ãˆæ¸ˆã¿ã®è¾æ›¸ã‚’ä½¿ç”¨\n",
    "        \"word_frequency_ranking\": [\n",
    "            {\"rank\": i, \"word\": word, \"count\": count} \n",
    "            for i, (word, count) in enumerate(sorted_word_count.items(), 1)\n",
    "        ],\n",
    "        \"total_words_analyzed\": len(keywords)\n",
    "    }\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"ğŸ“Š åˆ†æçµæœï¼ˆJSONå½¢å¼ï¼‰\")\n",
    "    print(\"=\"*50)\n",
    "    print(json.dumps(result, ensure_ascii=False, indent=2))\n",
    "    \n",
    "    # 11. çµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜\n",
    "    output_file = \"analysis_result.json\"\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(result, file, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"\\nâœ… çµæœã‚’ {output_file} ã«ä¿å­˜ã—ã¾ã—ãŸ\")\n",
    "    \n",
    "    # 12. ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã‚’ç”Ÿæˆ\n",
    "    create_wordcloud(sorted_word_count, top_n=10)\n",
    "    \n",
    "    print(\"ğŸ‰ åˆ†æå®Œäº†ï¼\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    analyze_text_with_bedrock()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f34e1d-474c-469c-a627-b2f98452a16e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
