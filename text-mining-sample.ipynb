{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "732f59b9-b946-4c95-9c93-487e5828b01f",
   "metadata": {},
   "source": [
    "# テキスト分析サンプルコード\n",
    "このノートブックでは AWS 環境を利用してテキスト分析を体験するためのサンプルコード集です。\n",
    "Amazon Comprehend、Amazon Bedrock、MeCab、Doc2Vecなどの様々な技術を使用して、日本語テキストの分析を行うサンプルコードを掲載しています。\n",
    "\n",
    "## 概要\n",
    "\n",
    "日本語テキストの自然言語処理において、以下の分析手法を比較・検証することを目的としています：\n",
    "\n",
    "* **Amazon Comprehend**: AWSの自然言語処理サービスを使用したキーフレーズ抽出、エンティティ検出、感情分析\n",
    "* **MeCab**: 日本語形態素解析による単語頻度分析\n",
    "* **Doc2Vec**: 文書ベクトル化による類似度分析\n",
    "*  **Amazon Bedrock**: Claude 4 Sonnetを使用した特徴的な単語抽出"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c6c3d1-888a-406f-b552-86b4781d1f04",
   "metadata": {},
   "source": [
    "必要なファイルを S3 からコピーする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4e54c3-5887-446e-b904-c370fc0d3a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp s3://text-analysis-hanson-2025/sample-text sample-text  --recursive --include \"*.md\"\n",
    "!aws s3 cp s3://text-analysis-hanson-2025/templete templete  --recursive --include \"*.md\"\n",
    "!aws s3 cp s3://text-analysis-hanson-2025/requirements.txt requirements.txt\n",
    "!aws s3 cp s3://text-analysis-hanson-2025/MEIRYO.TTC MEIRYO.TTC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d6083f-12d8-404c-8d6e-c5127d87c18e",
   "metadata": {},
   "source": [
    "必要なライブラリをインストールする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6603a425-53f9-4f9e-8565-e7bd9f878b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087fef27-fd1f-48ee-8e70-1709f3c18cb5",
   "metadata": {},
   "source": [
    "## Amazon Comprehend\n",
    "Python を使って、 Amazon Comprehend のテキスト分析を体験します\n",
    "\n",
    "Amazon Comprehend は、以下の種類のインサイトを分析します。\n",
    "\n",
    "* Entities — ドキュメント内での人物、場所、アイテム、場所への言及 (名前)。\n",
    "* Key phrases — ドキュメントに現れるフレーズ。例えば、バスケットボールの試合に関するドキュメントでは、チームの名前、会場の名前、最終スコアを返すことができます。\n",
    "* 個人を特定できる情報 (PII) — 住所や銀行口座番号、電話番号など、個人を特定できる個人データ。\n",
    "* Language — ドキュメントの主要言語。\n",
    "* Sentiment — ドキュメントの主要な感情。肯定的、中立、否定的、混在のいずれからにできます。\n",
    "* Targeted sentiment — ドキュメント内の特定のエンティティに関連する感情。出現する各エンティティに対する感情は、肯定的、否定的、中立、混在のいずれかにできます。\n",
    "* Syntax — ドキュメント内の各単語の品詞。\n",
    "\n",
    "なお、日本語の場合は `キーフレーズの検出` ・ `エンティティの検出` ・ `感情の分析` の3つにだけ対応しています。テキストファイル（ Markdown 形式）を読み込んで Amazon Comprehend に入力し、3パターンでの分析結果を出力するようなコードになっています。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e534750-b0a7-404d-93f6-a4ac14d9c28a",
   "metadata": {},
   "source": [
    "Amazon Comprehend 参考資料\n",
    "\n",
    "* デベロッパーガイド: [Amazon Comprehend とは](https://docs.aws.amazon.com/ja_jp/comprehend/latest/dg/what-is.html)\n",
    "* ハンズオン: [AWS Hands-on for Beginners: AWS Managed AI/ML サービス はじめの一歩](https://pages.awscloud.com/JAPAN-event-OE-Hands-on-for-Beginners-AIML-2022-confirmation_003.html)\n",
    "* 参考になるブログ: [【AWS】【機械学習】Amazon ComprehendのCustom classification (カスタム分類) を使って日本語のテキストを感情分析してみた](https://techblog.ap-com.co.jp/entry/2024/07/01/093000)\n",
    "* カスタムモデルの例: [Amazon Comprehend を使用してカスタムエンティティレコグナイザーを構築する](https://aws.amazon.com/jp/blogs/news/build-a-custom-entity-recognizer-using-amazon-comprehend/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8962cc-90ea-4050-a0b7-56b1a8ded8a4",
   "metadata": {},
   "source": [
    "まずは関数を定義しておきます"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4753d2fd-34fc-4610-8f0b-0bcbcae417bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "\n",
    "# --- 設定 ---\n",
    "# SageMaker Notebookインスタンスから実行する場合、ロールにComprehendへのアクセス権があれば\n",
    "# access_key, secret_key, region_name の設定は不要です。\n",
    "# boto3が自動的に認証情報を解決します。\n",
    "REGION_NAME = \"ap-northeast-1\" # 例: 東京リージョン\n",
    "\n",
    "\n",
    "# --- 関数定義 ---\n",
    "def analyze_text_with_comprehend(text, region_name):\n",
    "    \"\"\"\n",
    "    Amazon Comprehend を使ってテキスト分析を実行する関数。\n",
    "    - キーフレーズの検出\n",
    "    - エンティティの検出\n",
    "    - 感情の分析\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Comprehend クライアントの作成\n",
    "        comprehend = boto3.client(\"comprehend\", region_name=region_name)\n",
    "        language_code = 'ja'\n",
    "\n",
    "        # --- 1. キーフレーズ分析 ---\n",
    "        print(\"--- 1. Amazon Comprehendによるキーフレーズ分析 ---\")\n",
    "        key_phrases_response = comprehend.detect_key_phrases(Text=text, LanguageCode=language_code)\n",
    "        print(\"検出されたキーフレーズ:\")\n",
    "        if key_phrases_response['KeyPhrases']:\n",
    "            for phrase in key_phrases_response['KeyPhrases']:\n",
    "                print(f\"- {phrase['Text']} (スコア: {phrase['Score']:.4f})\")\n",
    "        else:\n",
    "            print(\"キーフレーズは見つかりませんでした。\")\n",
    "\n",
    "        print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "        # --- 2. エンティティ検出 ---\n",
    "        print(\"--- 2. Amazon Comprehendによるエンティティ検出 ---\")\n",
    "        entities_response = comprehend.detect_entities(Text=text, LanguageCode=language_code)\n",
    "        print(\"検出されたエンティティ:\")\n",
    "        if entities_response['Entities']:\n",
    "            for entity in entities_response['Entities']:\n",
    "                print(f\"- {entity['Text']} (タイプ: {entity['Type']}, スコア: {entity['Score']:.4f})\")\n",
    "        else:\n",
    "            print(\"エンティティは見つかりませんでした。\")\n",
    "\n",
    "        print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "        # --- 3. 感情分析 ---\n",
    "        print(\"--- 3. Amazon Comprehendによる感情分析 ---\")\n",
    "        sentiment_response = comprehend.detect_sentiment(Text=text, LanguageCode=language_code)\n",
    "        print(f\"全体の感情: {sentiment_response['Sentiment']}\")\n",
    "        print(\"感情スコア:\")\n",
    "        for sentiment, score in sentiment_response['SentimentScore'].items():\n",
    "            print(f\"- {sentiment}: {score:.4f}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"エラーが発生しました: {e}\")\n",
    "\n",
    "\n",
    "print(\"準備OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e3e531-17f9-4f5d-93f5-b9b0c82a355a",
   "metadata": {},
   "source": [
    "`準備OK` と出力されたら、分析できる状態になっています。もしエラーが表示される場合は、内容を確認してコードを修正しましょう。\n",
    "\n",
    "続いて `Amazon Comprehend` を使ったテキスト分析を実行します。最初は `sample-text/sample.md` を対象に分析するようになっていますが、慣れてきたら他のファイルも試してみましょう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d9931d-06ab-4e63-96bf-68e5b0ed946d",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # サンプルテキストファイルを読み込む\n",
    "    # 必要に応じてパスを調整してください。\n",
    "    file_path = \"sample-text/sample.md\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        sample_text = f.read()\n",
    "\n",
    "    # テキストが5000バイトを超えている場合は分割する必要があるが、今回はサンプルなので全体を一度に処理\n",
    "    if len(sample_text.encode('utf-8')) > 4800: # 念のため少し余裕を持たせる\n",
    "        print(\"警告: テキストサイズが大きいため、一部を切り詰めて分析します。\")\n",
    "        # UTF-8でエンコードしてからバイト数を基準に切り詰める\n",
    "        encoded_text = sample_text.encode('utf-8')\n",
    "        truncated_encoded_text = encoded_text[:4800]\n",
    "        # 不完全なマルチバイト文字で終わらないようにデコード・再エンコード\n",
    "        sample_text = truncated_encoded_text.decode('utf-8', 'ignore')\n",
    "\n",
    "    # Comprehendで分析を実行\n",
    "    analyze_text_with_comprehend(sample_text, REGION_NAME)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"エラー: ファイルが見つかりません。パスを確認してください: {file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"メイン処理でエラーが発生しました: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d035a371-43a7-4d14-a86f-a293c45fe76b",
   "metadata": {},
   "source": [
    "## MeCab\n",
    "\n",
    "次は、日本語形態素解析による単語頻度分析ができる `MeCab` を使ったテキスト分析を体験します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22773430-9fcf-4871-9462-efe86e8fb5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# うまく実行できない場合は、以下のコマンドを先頭の # を削除してから実行する\n",
    "# !python -m unidic download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be59186-19f6-4a4e-85dc-71969bbcbc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import MeCab\n",
    "import os\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# --- 設定 ---\n",
    "# MeCabの辞書パス。`!pip install ipadic` でインストールした場合、通常は自動で解決されますが、\n",
    "# 環境によっては明示的な指定が必要です。\n",
    "# 例: MECAB_ARGS = \"-d /usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd\"\n",
    "# ここではデフォルトの辞書(ipadic)を想定します。\n",
    "MECAB_ARGS = \"\"\n",
    "\n",
    "# 分析対象のサンプルファイル\n",
    "# ABSOLUTE_FILE_PATH = \"/workspaces/esio/amazon-comprehend/sample-text/sample.md\"\n",
    "\n",
    "# --- 関数定義 ---\n",
    "\n",
    "def analyze_word_frequency_with_mecab(text):\n",
    "    \"\"\"\n",
    "    MeCabを使ってテキストの単語出現頻度を分析する関数。\n",
    "    品詞フィルタリング、原型化、ストップワード除去などのベストプラクティスを適用します。\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # MeCabの初期化\n",
    "        tagger = MeCab.Tagger(MECAB_ARGS)\n",
    "        tagger.parse('') # UnicodeDecodeErrorを避けるためのおまじない\n",
    "\n",
    "        print(\"--- MeCabによる頻出単語分析（ベストプラクティス適用） ---\")\n",
    "        print(\"品詞フィルタリング、原型化、ストップワード除去などを行い、意味のある単語を抽出します。\")\n",
    "\n",
    "        node = tagger.parseToNode(text)\n",
    "        words = []\n",
    "        # ストップワードの定義（カタカナの原型も追加）\n",
    "        stop_words = ['こと', 'もの', 'ため', 'これ', 'それ', 'あれ', '私', 'よう', 'さん', \n",
    "                      'する', 'いる', 'なる', 'ある', 'いう', 'スル', 'イル']\n",
    "\n",
    "        while node:\n",
    "            # BOS/EOS (文頭・文末) や空のノードはスキップ\n",
    "            if node.surface == \"\":\n",
    "                node = node.next\n",
    "                continue\n",
    "\n",
    "            # 品詞情報などをカンマ区切りで取得\n",
    "            features = node.feature.split(',')\n",
    "            \n",
    "            # エラー回避: featuresの要素数が足りない場合はスキップ\n",
    "            if len(features) < 7:\n",
    "                node = node.next\n",
    "                continue\n",
    "\n",
    "            pos = features[0]         # 品詞\n",
    "            pos_detail1 = features[1] # 品詞細分類1\n",
    "            original_form = features[6] # 原型\n",
    "            surface_form = node.surface # 表層形\n",
    "\n",
    "            # 抽出対象の品詞を定義 (名詞、動詞、形容詞、副詞)\n",
    "            target_pos = ['名詞', '動詞', '形容詞', '副詞']\n",
    "\n",
    "            if pos in target_pos:\n",
    "                # 品詞によって原型を使うか表層形を使うか選択\n",
    "                if pos in ['動詞', '形容詞']:\n",
    "                    word_to_check = original_form\n",
    "                else: # 名詞、副詞など\n",
    "                    word_to_check = surface_form\n",
    "\n",
    "                # フィルタリング処理\n",
    "                is_valid = True\n",
    "                # 1. 除外する品詞細分類（名詞の場合）\n",
    "                if pos == '名詞' and pos_detail1 in ['非自立', '代名詞', '数', '接尾', '接続詞的']:\n",
    "                    is_valid = False\n",
    "                # 2. 単語が'*'や空の場合は除外\n",
    "                if word_to_check == '*' or not word_to_check:\n",
    "                    is_valid = False\n",
    "                # 3. ストップワードに含まれていれば除外\n",
    "                if word_to_check in stop_words:\n",
    "                    is_valid = False\n",
    "                # 4. 1文字のひらがな・カタカナは除外 (漢字やアルファベットは残す)\n",
    "                if len(word_to_check) == 1 and re.fullmatch(r'[ぁ-んァ-ヶ]', word_to_check):\n",
    "                    is_valid = False\n",
    "\n",
    "                if is_valid:\n",
    "                    words.append(word_to_check)\n",
    "            \n",
    "            node = node.next\n",
    "        \n",
    "        word_counts = Counter(words)\n",
    "        print(\"\\n最も頻繁に出現する意味のある単語 (トップ15):\")\n",
    "        if word_counts:\n",
    "            for word, count in word_counts.most_common(15):\n",
    "                print(f\"- {word}: {count}回\")\n",
    "        else:\n",
    "            print(\"分析対象の単語が見つかりませんでした。\")\n",
    "\n",
    "    except RuntimeError as e:\n",
    "        print(f\"MeCabの実行エラー: {e}\")\n",
    "        print(\"MeCabまたは辞書が正しくインストールされていない可能性があります。\")\n",
    "        print(\"ノートブックのセルで `!pip install mecab-python3 ipadic` を実行してください。\")\n",
    "    except Exception as e:\n",
    "        print(f\"エラーが発生しました: {e}\")\n",
    "\n",
    "# --- メイン処理 ---\n",
    "\n",
    "print(\"準備OK\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d62513-b024-4b25-bc64-273329642f70",
   "metadata": {},
   "source": [
    "`準備OK` と出力されたら、テキスト分析ができる状態になっています。\n",
    "\n",
    "次のセルを実行して、テキスト分析を体験しましょう。こちらも同様に、テキストファイル（マークダウン形式）を読み込んで分析するようになっていますので、慣れてきたら別のファイルでも試してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9fc6ac-2b82-4520-9ca6-7b1f1e8ce47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分析対象のサンプルファイル\n",
    "file_path = \"sample-text/sample.md\"\n",
    "\n",
    "\n",
    "\"\"\"テキストファイルを分析\"\"\"\n",
    "try:\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        sample_text = f.read()\n",
    "    \n",
    "    analyze_word_frequency_with_mecab(sample_text)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"エラー: ファイルが見つかりません。パスを確認してください: {file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"メイン処理でエラーが発生しました: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cde6ab-32b3-4f5d-b7d7-9a098dac4eac",
   "metadata": {},
   "source": [
    "## doc2vec\n",
    "### doc2vec とは\n",
    "\n",
    "Doc2Vecは任意の長さの文章を固定長のベクトルに変換する技術です。\n",
    "Word2Vecが単語の分散表現を獲得するものだったのに対し、Doc2Vecは文章や文書の分散表現を獲得します。文章の分散表現を獲得する手法としては古典的なものとしてはBag-of-WordsやTF-IDFがありますが、それらは下記のような弱点を有しています。\n",
    "\n",
    "文章内の単語の語順情報を有していない\n",
    "同義語でも完全に異なる独立した単語として認識する\n",
    "これらはカウントベースと呼ばれる手法ですが、Doc2Vecは上記弱点を克服すべく違うアプローチで文章の分散表現の獲得を試みています。\n",
    "\n",
    "> 引用: [(Qiita) Doc2Vecについてまとめる](https://qiita.com/g-k/items/5ea94c13281f675302ca#doc2vec%E3%81%A8%E3%81%AF%E4%BD%95%E3%81%8B)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564d56a0-ed20-4594-868f-18e56cf47648",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import MeCab\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "from collections import Counter\n",
    "import json\n",
    "\n",
    "\n",
    "# --- 設定 ---\n",
    "MECAB_ARGS = \"\"\n",
    "SAMPLE_DIR_PATH = \"sample-text/\"    # ディレクトリ内の全ての文書を対象に分析します\n",
    "\n",
    "# ハンズオン向け設定\n",
    "MAX_FILES = 5  # 適度なファイル数\n",
    "MAX_TEXT_LENGTH = 3000  # 十分な分析データ\n",
    "EPOCHS = 15  # バランスの取れたエポック数\n",
    "MIN_COUNT = 2\n",
    "VECTOR_SIZE = 100\n",
    "\n",
    "\n",
    "class Doc2VecAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.documents = []\n",
    "        self.document_names = []\n",
    "        self.word_stats = {}\n",
    "        \n",
    "    def preprocess_text(self, text, doc_name):\n",
    "        \"\"\"\n",
    "        高品質な前処理：統計情報も収集\n",
    "        \"\"\"\n",
    "        if len(text) > MAX_TEXT_LENGTH:\n",
    "            text = text[:MAX_TEXT_LENGTH]\n",
    "        \n",
    "        tagger = MeCab.Tagger(MECAB_ARGS)\n",
    "        tagger.parse('')\n",
    "        node = tagger.parseToNode(text)\n",
    "        \n",
    "        words = []\n",
    "        pos_stats = Counter()\n",
    "        \n",
    "        # 拡張されたstop wordsリスト\n",
    "        stop_words = {\n",
    "            # 基本的な機能語\n",
    "            'こと', 'もの', 'ため', 'これ', 'それ', 'あれ', 'どれ', 'よう', 'さん', 'ところ',\n",
    "            'とき', 'とこ', 'など', 'なに', 'なん', 'どこ', 'いつ', 'だれ', 'どう', 'なぜ',\n",
    "            \n",
    "            # 代名詞・指示語\n",
    "            '私', '僕', '俺', '君', '彼', '彼女', 'あなた', 'みなさん', 'みんな',\n",
    "            'ここ', 'そこ', 'あそこ', 'どこか', 'いま', 'いつか', 'どこでも',\n",
    "            \n",
    "            # 助詞的な名詞\n",
    "            '上', '下', '中', '前', '後', '左', '右', '横', '隣', '間', '内', '外',\n",
    "            '先', '奥', '手前', '向こう', '以上', '以下', '未満', '程度', '以外',\n",
    "            \n",
    "            # 時間・頻度表現\n",
    "            '今日', '昨日', '明日', '今年', '去年', '来年', '今月', '先月', '来月',\n",
    "            '毎日', '毎回', '毎年', '毎月', '毎週', '常に', 'いつも', 'たまに',\n",
    "            \n",
    "            # 数量・程度表現\n",
    "            '全て', '全部', 'すべて', '一部', '半分', '大部分', '少し', 'ちょっと',\n",
    "            'かなり', 'とても', 'すごく', 'めちゃくちゃ', '非常', '極めて',\n",
    "            \n",
    "            # 接続・転換表現\n",
    "            'しかし', 'でも', 'だが', 'ただし', 'ところが', 'けれど', 'けれども',\n",
    "            'そして', 'また', 'さらに', 'それから', 'それで', 'そこで', 'つまり',\n",
    "            \n",
    "            # 感嘆・応答表現\n",
    "            'はい', 'いいえ', 'ええ', 'うん', 'そう', 'そうです', 'なるほど',\n",
    "            'おお', 'ああ', 'うーん', 'えー', 'まあ', 'やはり', 'やっぱり',\n",
    "            \n",
    "            # 一般的すぎる動詞・形容詞の語幹\n",
    "            'する', 'なる', 'ある', 'いる', 'できる', 'みる', 'いく', 'くる',\n",
    "            'いい', 'よい', '悪い', '大きい', '小さい', '新しい', '古い',\n",
    "            \n",
    "            # ビジネス・技術文書でよく出る一般語\n",
    "            '場合', '状況', '状態', '方法', '手段', '方式', '形式', '種類', '方向',\n",
    "            '結果', '効果', '影響', '関係', '関連', '対象', '目的', '理由', '原因',\n",
    "            '問題', '課題', '解決', '改善', '向上', '発展', '進歩', '変化', '変更',\n",
    "            \n",
    "            # 単位・助数詞的表現\n",
    "            '個', '本', '枚', '台', '人', '回', '度', '倍', '割', 'パーセント',\n",
    "            '時間', '分', '秒', '日', '週間', 'ヶ月', '年間', 'メートル', 'キロ'\n",
    "        }\n",
    "\n",
    "        while node:\n",
    "            if not node.surface:\n",
    "                node = node.next\n",
    "                continue\n",
    "\n",
    "            features = node.feature.split(',')\n",
    "            if len(features) < 7:\n",
    "                node = node.next\n",
    "                continue\n",
    "\n",
    "            pos = features[0]\n",
    "            pos_detail1 = features[1]\n",
    "            original_form = features[6]\n",
    "            surface_form = node.surface\n",
    "\n",
    "            # 統計収集\n",
    "            pos_stats[pos] += 1\n",
    "\n",
    "            # 高品質な品詞選択\n",
    "            target_pos = ['名詞', '動詞', '形容詞']\n",
    "            if pos in target_pos:\n",
    "                word_to_check = original_form if pos in ['動詞', '形容詞'] else surface_form\n",
    "                \n",
    "                # 高品質フィルタリング\n",
    "                if (word_to_check != '*' and \n",
    "                    word_to_check not in stop_words and \n",
    "                    len(word_to_check) > 1 and\n",
    "                    not re.match(r'^[0-9]+$', word_to_check) and  # 数字のみ除外\n",
    "                    pos != '名詞' or pos_detail1 not in ['数', '非自立', '代名詞']):\n",
    "                    words.append(word_to_check)\n",
    "\n",
    "            node = node.next\n",
    "        \n",
    "        # 文書統計を保存\n",
    "        self.word_stats[doc_name] = {\n",
    "            'total_words': len(words),\n",
    "            'unique_words': len(set(words)),\n",
    "            'pos_distribution': dict(pos_stats.most_common(5))\n",
    "        }\n",
    "        \n",
    "        return words\n",
    "\n",
    "    def train_model(self, documents, document_names):\n",
    "        \"\"\"\n",
    "        Doc2Vecモデルの学習\n",
    "        \"\"\"\n",
    "        print(\"=== Doc2Vec モデル学習フェーズ ===\")\n",
    "        \n",
    "        tagged_documents = [\n",
    "            TaggedDocument(doc, [name]) \n",
    "            for doc, name in zip(documents, document_names)\n",
    "        ]\n",
    "        \n",
    "        self.model = Doc2Vec(\n",
    "            vector_size=VECTOR_SIZE,\n",
    "            min_count=MIN_COUNT,\n",
    "            epochs=EPOCHS,\n",
    "            workers=min(4, os.cpu_count()),\n",
    "            dm=1,  # PV-DM (分散メモリ)\n",
    "            window=5,\n",
    "            alpha=0.025,\n",
    "            min_alpha=0.00025\n",
    "        )\n",
    "        \n",
    "        start_time = time.time()\n",
    "        self.model.build_vocab(tagged_documents)\n",
    "        \n",
    "        print(f\"学習開始: {len(documents)}文書, 語彙数: {len(self.model.wv.key_to_index)}\")\n",
    "        self.model.train(tagged_documents, total_examples=self.model.corpus_count, epochs=self.model.epochs)\n",
    "        \n",
    "        training_time = time.time() - start_time\n",
    "        print(f\"学習完了 (所要時間: {training_time:.2f}秒)\")\n",
    "        \n",
    "        return self.model\n",
    "\n",
    "    def analyze_document_similarity(self):\n",
    "        \"\"\"\n",
    "        文書間類似度分析\n",
    "        \"\"\"\n",
    "        print(\"\\n=== 文書間類似度分析 ===\")\n",
    "        \n",
    "        if len(self.document_names) < 2:\n",
    "            print(\"類似度分析には2つ以上の文書が必要です。\")\n",
    "            return\n",
    "        \n",
    "        similarities = []\n",
    "        for i, doc1 in enumerate(self.document_names):\n",
    "            for j, doc2 in enumerate(self.document_names[i+1:], i+1):\n",
    "                try:\n",
    "                    similarity = self.model.dv.similarity(doc1, doc2)\n",
    "                    similarities.append((doc1, doc2, similarity))\n",
    "                    print(f\"{os.path.basename(doc1)} ⟷ {os.path.basename(doc2)}: {similarity:.4f}\")\n",
    "                except KeyError as e:\n",
    "                    print(f\"文書ベクトルが見つかりません: {e}\")\n",
    "        \n",
    "        # 最も類似した文書ペア\n",
    "        if similarities:\n",
    "            most_similar = max(similarities, key=lambda x: x[2])\n",
    "            print(f\"\\n最も類似した文書ペア:\")\n",
    "            print(f\"  {os.path.basename(most_similar[0])} ⟷ {os.path.basename(most_similar[1])}\")\n",
    "            print(f\"  類似度: {most_similar[2]:.4f}\")\n",
    "\n",
    "    def analyze_word_similarity(self):\n",
    "        \"\"\"\n",
    "        単語類似度分析（教育的な解説付き）\n",
    "        \"\"\"\n",
    "        print(\"\\n=== 単語類似度分析 ===\")\n",
    "        \n",
    "        # 語彙から頻出単語を選択\n",
    "        word_freq = Counter()\n",
    "        for doc in self.documents:\n",
    "            word_freq.update(doc)\n",
    "        \n",
    "        common_words = [word for word, freq in word_freq.most_common(20) \n",
    "                       if word in self.model.wv.key_to_index]\n",
    "        \n",
    "        if not common_words:\n",
    "            print(\"分析可能な単語が見つかりません。\")\n",
    "            return\n",
    "        \n",
    "        print(f\"頻出単語から分析対象を選択: {common_words[:5]}\")\n",
    "        \n",
    "        for word in common_words[:3]:\n",
    "            print(f\"\\n'{word}' の類似単語:\")\n",
    "            try:\n",
    "                similar_words = self.model.wv.most_similar(word, topn=5)\n",
    "                for i, (sim_word, similarity) in enumerate(similar_words, 1):\n",
    "                    print(f\"  {i}. {sim_word} (類似度: {similarity:.4f})\")\n",
    "            except KeyError:\n",
    "                print(f\"  '{word}' の類似単語を見つけられませんでした。\")\n",
    "\n",
    "    def analyze_word_clusters(self):\n",
    "        \"\"\"\n",
    "        単語クラスタリング分析\n",
    "        \"\"\"\n",
    "        print(\"\\n=== 単語クラスタリング分析 ===\")\n",
    "        \n",
    "        # 頻出単語のベクトルを取得\n",
    "        word_freq = Counter()\n",
    "        for doc in self.documents:\n",
    "            word_freq.update(doc)\n",
    "        \n",
    "        target_words = [word for word, freq in word_freq.most_common(15) \n",
    "                       if word in self.model.wv.key_to_index]\n",
    "        \n",
    "        if len(target_words) < 3:\n",
    "            print(\"クラスタリングに十分な単語がありません。\")\n",
    "            return\n",
    "        \n",
    "        print(f\"分析対象単語: {target_words}\")\n",
    "        \n",
    "        # 簡易クラスタリング（類似度ベース）\n",
    "        clusters = {}\n",
    "        processed = set()\n",
    "        \n",
    "        for word in target_words:\n",
    "            if word in processed:\n",
    "                continue\n",
    "                \n",
    "            cluster = [word]\n",
    "            processed.add(word)\n",
    "            \n",
    "            try:\n",
    "                similar_words = self.model.wv.most_similar(word, topn=10)\n",
    "                for sim_word, similarity in similar_words:\n",
    "                    if sim_word in target_words and similarity > 0.3 and sim_word not in processed:\n",
    "                        cluster.append(sim_word)\n",
    "                        processed.add(sim_word)\n",
    "                \n",
    "                if len(cluster) > 1:\n",
    "                    clusters[f\"クラスタ_{len(clusters)+1}\"] = cluster\n",
    "            except KeyError:\n",
    "                continue\n",
    "        \n",
    "        print(\"\\n単語クラスタ:\")\n",
    "        for cluster_name, words in clusters.items():\n",
    "            print(f\"  {cluster_name}: {', '.join(words)}\")\n",
    "\n",
    "    def generate_analysis_report(self):\n",
    "        \"\"\"\n",
    "        分析レポートの生成\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"=== Doc2Vec 分析レポート ===\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        print(f\"\\n【モデル情報】\")\n",
    "        print(f\"  ベクトル次元数: {self.model.vector_size}\")\n",
    "        print(f\"  語彙数: {len(self.model.wv.key_to_index)}\")\n",
    "        print(f\"  学習エポック数: {self.model.epochs}\")\n",
    "        \n",
    "        print(f\"\\n【文書統計】\")\n",
    "        for doc_name, stats in self.word_stats.items():\n",
    "            print(f\"  {os.path.basename(doc_name)}:\")\n",
    "            print(f\"    総単語数: {stats['total_words']}\")\n",
    "            print(f\"    ユニーク単語数: {stats['unique_words']}\")\n",
    "            print(f\"    語彙多様性: {stats['unique_words']/stats['total_words']:.3f}\")\n",
    "        \n",
    "        print(f\"\\n【頻出単語トップ10】\")\n",
    "        word_freq = Counter()\n",
    "        for doc in self.documents:\n",
    "            word_freq.update(doc)\n",
    "        \n",
    "        for i, (word, freq) in enumerate(word_freq.most_common(10), 1):\n",
    "            print(f\"  {i:2d}. {word} ({freq}回)\")\n",
    "\n",
    "\n",
    "print(\"準備OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16cc468-293d-41ac-9e0f-d804a6c7590e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    メイン処理\n",
    "    \"\"\"\n",
    "    analyzer = Doc2VecAnalyzer()\n",
    "\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "\n",
    "        # ファイル読み込み\n",
    "        search_path = os.path.join(SAMPLE_DIR_PATH, '*.md')\n",
    "        file_paths = glob.glob(search_path)\n",
    "\n",
    "        if not file_paths:\n",
    "            print(f\"エラー: サンプル文書が見つかりません: {SAMPLE_DIR_PATH}\")\n",
    "            return\n",
    "\n",
    "        # ファイル数制限\n",
    "        limited_files = file_paths[:MAX_FILES]\n",
    "        print(f\"=== ファイル読み込み ===\")\n",
    "        print(f\"対象ファイル数: {len(limited_files)}\")\n",
    "\n",
    "        documents = []\n",
    "        document_names = []\n",
    "\n",
    "        for i, path in enumerate(limited_files):\n",
    "            filename = os.path.basename(path)\n",
    "            print(f\"処理中: {filename} ({i+1}/{len(limited_files)})\")\n",
    "\n",
    "            with open(path, 'r', encoding='utf-8') as f:\n",
    "                text = f.read()\n",
    "\n",
    "            words = analyzer.preprocess_text(text, path)\n",
    "            if words:\n",
    "                documents.append(words)\n",
    "                document_names.append(path)\n",
    "                print(f\"  抽出単語数: {len(words)}\")\n",
    "\n",
    "        if not documents:\n",
    "            print(\"エラー: 分析可能な文書がありません。\")\n",
    "            return\n",
    "\n",
    "        analyzer.documents = documents\n",
    "        analyzer.document_names = document_names\n",
    "\n",
    "        # モデル学習\n",
    "        analyzer.train_model(documents, document_names)\n",
    "\n",
    "        # 各種分析実行\n",
    "        analyzer.analyze_document_similarity()\n",
    "        analyzer.analyze_word_similarity()\n",
    "        analyzer.analyze_word_clusters()\n",
    "        analyzer.generate_analysis_report()\n",
    "\n",
    "        total_time = time.time() - start_time\n",
    "        print(f\"\\n総処理時間: {total_time:.2f}秒\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"エラーが発生しました: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8a98cf-fdcb-49b4-93af-c5e3faa362d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b071575-2863-4b98-b38c-be2b5bf9660f",
   "metadata": {},
   "source": [
    "# 生成AIを使用する場合\n",
    "ここからは、生成AIを使用した自然言語処理のサンプルコードです。\n",
    "\n",
    "[Amazon Bedrock](https://aws.amazon.com/jp/bedrock/) を使用すると、様々な生成AIモデルを利用することができます。今回は、 `Anthropic` が提供するテキスト系の LLM である `Claude 4 Sonnet` を利用して、テキスト分析を実施します。\n",
    "\n",
    "分析のステップは以下のとおりです。\n",
    "\n",
    "1. LLM で特徴的な単語を抽出してリストとして取得する\n",
    "2. LLM のレスポンスに不要なテキストが含まれる可能性があるので、必要なレスポンス(json)だけをパースする。\n",
    "3. リストの特徴後を1つずつ、元のテキスト内でカウントする。 >>> 結果: `analysis_result.json` \n",
    "4. (おまけ) 出現回数が多いワードを、ワードクラウドで可視化する >>> 結果: `wordcloud_result.png`\n",
    "\n",
    "(参考)\n",
    "LLM 用のプロンプトは `templete/llm.md` にあります。 GenU のビルダーモードでも利用できるように作成されています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2e24cd-2d74-4ccb-b2d9-8971a20bbaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import re\n",
    "from datetime import datetime\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def extract_json_from_text(text):\n",
    "    \"\"\"\n",
    "    テキストからJSONを抽出する関数\n",
    "    LLMの出力にJSON以外のテキストが含まれている場合に対応\n",
    "    \"\"\"\n",
    "    # 複数のパターンでJSONを探す\n",
    "    patterns = [\n",
    "        # パターン1: 最初の{から最後の}まで\n",
    "        r'\\{.*\\}',\n",
    "        # パターン2: ```json ブロック内\n",
    "        r'```json\\s*(\\{.*?\\})\\s*```',\n",
    "        # パターン3: ``` ブロック内\n",
    "        r'```\\s*(\\{.*?\\})\\s*```'\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        matches = re.findall(pattern, text, re.DOTALL)\n",
    "        for match in matches:\n",
    "            try:\n",
    "                # JSONとして解析を試行\n",
    "                parsed = json.loads(match)\n",
    "                return parsed\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "    \n",
    "    # 行ごとに分割してJSONらしい行を探す\n",
    "    lines = text.split('\\n')\n",
    "    json_lines = []\n",
    "    in_json = False\n",
    "    brace_count = 0\n",
    "    \n",
    "    for line in lines:\n",
    "        stripped = line.strip()\n",
    "        if stripped.startswith('{'):\n",
    "            in_json = True\n",
    "            json_lines = [line]\n",
    "            brace_count = line.count('{') - line.count('}')\n",
    "        elif in_json:\n",
    "            json_lines.append(line)\n",
    "            brace_count += line.count('{') - line.count('}')\n",
    "            if brace_count <= 0:\n",
    "                break\n",
    "    \n",
    "    if json_lines:\n",
    "        try:\n",
    "            json_text = '\\n'.join(json_lines)\n",
    "            return json.loads(json_text)\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "    \n",
    "    return None\n",
    "\n",
    "def create_wordcloud(word_frequency, top_n=10):\n",
    "    \"\"\"\n",
    "    単語の出現回数からワードクラウドを生成する関数\n",
    "    \"\"\"\n",
    "    # 上位N位までの単語を取得\n",
    "    top_words = dict(list(word_frequency.items())[:top_n])\n",
    "    \n",
    "    if not top_words:\n",
    "        print(\"❌ ワードクラウド用のデータがありません\")\n",
    "        return\n",
    "    \n",
    "    print(f\"🎨 上位{top_n}位までの単語でワードクラウドを生成中...\")\n",
    "    \n",
    "    try:\n",
    "        # 日本語フォントファイルのパスを指定\n",
    "        font_path = \"./MEIRYO.TTC\"\n",
    "        \n",
    "        # ワードクラウドを生成\n",
    "        wordcloud = WordCloud(\n",
    "            width=800,\n",
    "            height=400,\n",
    "            background_color='white',\n",
    "            max_words=top_n,\n",
    "            relative_scaling=0.5,\n",
    "            colormap='viridis',\n",
    "            font_path=font_path  # MEIRYOフォントを指定\n",
    "        ).generate_from_frequencies(top_words)\n",
    "        \n",
    "        # プロット設定（日本語フォント対応）\n",
    "        plt.rcParams['font.family'] = 'DejaVu Sans'  # 英語部分用\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.axis('off')\n",
    "        plt.title(f'Word Frequency Ranking Top {top_n}', fontsize=16, pad=20)\n",
    "        \n",
    "        # ファイルに保存\n",
    "        output_file = \"wordcloud_result.png\"\n",
    "        plt.savefig(output_file, bbox_inches='tight', dpi=300)\n",
    "        print(f\"✅ ワードクラウドを {output_file} に保存しました\")\n",
    "        \n",
    "        # 画面に表示\n",
    "        plt.show()\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(\"❌ MEIRYO.TTCフォントファイルが見つかりません\")\n",
    "        print(\"💡 カレントディレクトリにMEIRYO.TTCファイルがあることを確認してください\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ ワードクラウド生成でエラーが発生しました: {e}\")\n",
    "\n",
    "\n",
    "print(\"準備OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5722a8-17bb-458a-a93a-33b32802658a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_text_with_bedrock():\n",
    "    \"\"\"\n",
    "    メイン分析関数：テキストファイルを読み込み、Bedrockで分析し、結果を出力\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. 設定\n",
    "    # Claude 4 Sonnet用の推論プロファイルIDを使用\n",
    "    MODEL_ID = \"us.anthropic.claude-sonnet-4-20250514-v1:0\"\n",
    "    REGION = \"us-east-1\"\n",
    "    \n",
    "    print(\"🚀 Amazon Bedrock テキスト分析を開始します...\")\n",
    "    \n",
    "    # 2. テキストファイルを読み込み\n",
    "    try:\n",
    "        with open(\"sample-text/sample.md\", \"r\", encoding=\"utf-8\") as file:\n",
    "            sample_text = file.read()\n",
    "        print(\"✅ サンプルテキストを読み込みました\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"❌ sample.mdファイルが見つかりません\")\n",
    "        return\n",
    "    \n",
    "    # 3. プロンプトテンプレートを読み込み\n",
    "    try:\n",
    "        with open(\"templete/llm.md\", \"r\", encoding=\"utf-8\") as file:\n",
    "            prompt_template = file.read()\n",
    "        print(\"✅ プロンプトテンプレートを読み込みました\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"❌ llm.mdファイルが見つかりません\")\n",
    "        return\n",
    "    \n",
    "    # 4. Bedrockクライアントを作成\n",
    "    bedrock_runtime = boto3.client(\n",
    "        service_name=\"bedrock-runtime\",\n",
    "        region_name=REGION\n",
    "    )\n",
    "    print(\"✅ Bedrockクライアントを初期化しました\")\n",
    "    \n",
    "    # 5. プロンプトを作成（テンプレートにテキストを埋め込み）\n",
    "    prompt = prompt_template.replace(\"{{解析対象のテキストをLLMに渡す}}\", sample_text)\n",
    "    \n",
    "    # 6. Bedrock converse_stream APIを呼び出し\n",
    "    try:\n",
    "        print(\"🤖 Claude 4 Sonnet で分析中...\")\n",
    "        \n",
    "        response = bedrock_runtime.converse_stream(\n",
    "            modelId=MODEL_ID,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            \"text\": prompt\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            ],\n",
    "            inferenceConfig={\n",
    "                \"maxTokens\": 2000,\n",
    "                \"temperature\": 0.1\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # ストリーミングレスポンスを収集\n",
    "        llm_output = \"\"\n",
    "        for event in response[\"stream\"]:\n",
    "            if \"contentBlockDelta\" in event:\n",
    "                delta = event[\"contentBlockDelta\"][\"delta\"]\n",
    "                if \"text\" in delta:\n",
    "                    llm_output += delta[\"text\"]\n",
    "        \n",
    "        print(\"✅ LLMからの応答を取得しました\")\n",
    "        print(f\"📄 LLM出力プレビュー: {llm_output[:200]}...\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Bedrock API呼び出しでエラーが発生しました: {e}\")\n",
    "        return\n",
    "    \n",
    "    # 7. LLMの出力からJSONを抽出（堅牢なパース処理）\n",
    "    try:\n",
    "        print(\"🔍 JSON形式のデータを抽出中...\")\n",
    "        \n",
    "        # 堅牢なJSON抽出を実行\n",
    "        keywords_data = extract_json_from_text(llm_output)\n",
    "        \n",
    "        if keywords_data is None:\n",
    "            print(\"❌ 有効なJSONが見つかりませんでした\")\n",
    "            print(f\"LLM完全出力:\\n{llm_output}\")\n",
    "            return\n",
    "        \n",
    "        # resultsキーから単語配列を取得\n",
    "        if \"results\" not in keywords_data:\n",
    "            print(\"❌ 'results'キーが見つかりません\")\n",
    "            print(f\"取得したJSON: {keywords_data}\")\n",
    "            return\n",
    "            \n",
    "        keywords = keywords_data[\"results\"]\n",
    "        print(f\"✅ {len(keywords)}個の特徴的な単語を抽出しました\")\n",
    "        print(f\"📝 抽出された単語: {keywords}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ JSON解析でエラーが発生しました: {e}\")\n",
    "        print(f\"LLM完全出力:\\n{llm_output}\")\n",
    "        return\n",
    "    \n",
    "    # 8. 各単語の出現回数をカウント\n",
    "    print(\"🔍 単語の出現回数をカウント中...\")\n",
    "    \n",
    "    word_count = {}\n",
    "    for keyword in keywords:\n",
    "        # 正規表現で単語の出現回数をカウント（大文字小文字を区別しない）\n",
    "        pattern = re.escape(keyword)\n",
    "        matches = re.findall(pattern, sample_text, re.IGNORECASE)\n",
    "        count = len(matches)\n",
    "        word_count[keyword] = count\n",
    "        print(f\"  📝 '{keyword}': {count}回\")\n",
    "    \n",
    "    # 9. 出現回数の多い順に並び替え\n",
    "    print(\"📊 出現回数の多い順に並び替え中...\")\n",
    "    sorted_word_count = dict(sorted(word_count.items(), key=lambda x: x[1], reverse=True))\n",
    "    \n",
    "    print(\"🏆 出現回数ランキング:\")\n",
    "    for i, (word, count) in enumerate(sorted_word_count.items(), 1):\n",
    "        print(f\"  {i}位: '{word}' - {count}回\")\n",
    "    \n",
    "    # 10. 結果をJSON形式で出力\n",
    "    result = {\n",
    "        \"analysis_timestamp\": datetime.now().isoformat(),\n",
    "        \"source_file\": \"sample-text/sample.md\",\n",
    "        \"model_used\": MODEL_ID,\n",
    "        \"extracted_keywords\": keywords,\n",
    "        \"word_frequency\": sorted_word_count,  # 並び替え済みの辞書を使用\n",
    "        \"word_frequency_ranking\": [\n",
    "            {\"rank\": i, \"word\": word, \"count\": count} \n",
    "            for i, (word, count) in enumerate(sorted_word_count.items(), 1)\n",
    "        ],\n",
    "        \"total_words_analyzed\": len(keywords)\n",
    "    }\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"📊 分析結果（JSON形式）\")\n",
    "    print(\"=\"*50)\n",
    "    print(json.dumps(result, ensure_ascii=False, indent=2))\n",
    "    \n",
    "    # 11. 結果をファイルに保存\n",
    "    output_file = \"analysis_result.json\"\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(result, file, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"\\n✅ 結果を {output_file} に保存しました\")\n",
    "    \n",
    "    # 12. ワードクラウドを生成\n",
    "    create_wordcloud(sorted_word_count, top_n=10)\n",
    "    \n",
    "    print(\"🎉 分析完了！\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    analyze_text_with_bedrock()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f34e1d-474c-469c-a627-b2f98452a16e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
